{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic\n",
    "import pydantic_ai\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIModel(\n",
    "    model_name='dolphin-mixtral:8x7b', \n",
    "    provider=OpenAIProvider(base_url='http://localhost:11434/v1'),\n",
    ")\n",
    "agent = pydantic_ai.Agent(\n",
    "    model=model,\n",
    "    system_prompt=\"Answer any questions the user has!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentRunResult(data=' The capital city of France is Paris. It is also one of Europe\\'s major centers for business, politics and culture. Known as \"La Ville LumiÃ¨re\" or \"The City of Light,\" it is famous for its beauty, architecture, art, food and fashion. The Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral are some of the significant attractions in Paris.')\n"
     ]
    }
   ],
   "source": [
    "result = await agent.run(\n",
    "    user_prompt=\"What is the capital of France?\",\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/collections/__init__.py:449: RuntimeWarning: coroutine 'Agent.run' was never awaited\n",
      "  result = tuple_new(cls, iterable)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "ModelHTTPError",
     "evalue": "status_code: 400, model_name: dolphin-mixtral:8x7b, body: {'message': 'registry.ollama.ai/library/dolphin-mixtral:8x7b does not support tools', 'type': 'api_error', 'param': None, 'code': None}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic_ai/models/openai.py:269\u001b[0m, in \u001b[0;36mOpenAIModel._completions_create\u001b[0;34m(self, messages, stream, model_settings, model_request_parameters)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    270\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_name,\n\u001b[1;32m    271\u001b[0m         messages\u001b[38;5;241m=\u001b[39mopenai_messages,\n\u001b[1;32m    272\u001b[0m         n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    273\u001b[0m         parallel_tool_calls\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    274\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[1;32m    275\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[1;32m    276\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    277\u001b[0m         stream_options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_usage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m NOT_GIVEN,\n\u001b[1;32m    278\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    279\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    280\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    281\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    282\u001b[0m         seed\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    283\u001b[0m         presence_penalty\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    284\u001b[0m         frequency_penalty\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    285\u001b[0m         logit_bias\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    286\u001b[0m         reasoning_effort\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_reasoning_effort\u001b[39m\u001b[38;5;124m'\u001b[39m, NOT_GIVEN),\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APIStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1927\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1926\u001b[0m validate_response_format(response_format)\n\u001b[0;32m-> 1927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1928\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1929\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1930\u001b[0m         {\n\u001b[1;32m   1931\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1932\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1933\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1934\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1935\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1936\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1937\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1938\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1939\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1940\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1942\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1943\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1944\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1945\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   1946\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1947\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   1948\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1949\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1950\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1951\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1953\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1955\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1956\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1957\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1958\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1959\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1960\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1961\u001b[0m         },\n\u001b[1;32m   1962\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1963\u001b[0m     ),\n\u001b[1;32m   1964\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1965\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1966\u001b[0m     ),\n\u001b[1;32m   1967\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1968\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1969\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1970\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:1767\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1764\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1765\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1766\u001b[0m )\n\u001b[0;32m-> 1767\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:1461\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1462\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1463\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1464\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1465\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1466\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1467\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/openai/_base_client.py:1562\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1565\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1566\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1570\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1571\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'registry.ollama.ai/library/dolphin-mixtral:8x7b does not support tools', 'type': 'api_error', 'param': None, 'code': None}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mModelHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     answer: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m pydantic\u001b[38;5;241m.\u001b[39mField(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe answer to the user\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms question.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m     reasons: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m pydantic\u001b[38;5;241m.\u001b[39mField(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe reasons for the answer.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m      6\u001b[0m     user_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     result_type\u001b[38;5;241m=\u001b[39mOutputFormat,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic_ai/agent.py:316\u001b[0m, in \u001b[0;36mAgent.run\u001b[0;34m(self, user_prompt, result_type, message_history, model, deps, model_settings, usage_limits, usage, infer_name)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_name(inspect\u001b[38;5;241m.\u001b[39mcurrentframe())\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(\n\u001b[1;32m    307\u001b[0m     user_prompt\u001b[38;5;241m=\u001b[39muser_prompt,\n\u001b[1;32m    308\u001b[0m     result_type\u001b[38;5;241m=\u001b[39mresult_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m     usage\u001b[38;5;241m=\u001b[39musage,\n\u001b[1;32m    315\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m agent_run:\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m agent_run:\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (final_result \u001b[38;5;241m:=\u001b[39m agent_run\u001b[38;5;241m.\u001b[39mresult) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe graph run did not finish properly\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic_ai/agent.py:1352\u001b[0m, in \u001b[0;36mAgentRun.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__anext__\u001b[39m(\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1350\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _agent_graph\u001b[38;5;241m.\u001b[39mAgentNode[AgentDepsT, ResultDataT] \u001b[38;5;241m|\u001b[39m End[FinalResult[ResultDataT]]:\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1352\u001b[0m     next_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_run\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__anext__\u001b[39m()\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _agent_graph\u001b[38;5;241m.\u001b[39mis_agent_node(next_node):\n\u001b[1;32m   1354\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m next_node\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic_graph/graph.py:734\u001b[0m, in \u001b[0;36mGraphRun.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_node, End):\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_node)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic_graph/graph.py:723\u001b[0m, in \u001b[0;36mGraphRun.next\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    720\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n\u001b[1;32m    721\u001b[0m deps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeps\n\u001b[0;32m--> 723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mnext(node, history, state\u001b[38;5;241m=\u001b[39mstate, deps\u001b[38;5;241m=\u001b[39mdeps, infer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_node\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic_graph/graph.py:305\u001b[0m, in \u001b[0;36mGraph.next\u001b[0;34m(self, node, history, state, deps, infer_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m     start_ts \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mnow_utc()\n\u001b[1;32m    304\u001b[0m     start \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[0;32m--> 305\u001b[0m     next_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m node\u001b[38;5;241m.\u001b[39mrun(ctx)\n\u001b[1;32m    306\u001b[0m     duration \u001b[38;5;241m=\u001b[39m perf_counter() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m    308\u001b[0m history\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    309\u001b[0m     NodeStep(state\u001b[38;5;241m=\u001b[39mstate, node\u001b[38;5;241m=\u001b[39mnode, start_ts\u001b[38;5;241m=\u001b[39mstart_ts, duration\u001b[38;5;241m=\u001b[39mduration, snapshot_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnapshot_state)\n\u001b[1;32m    310\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:252\u001b[0m, in \u001b[0;36mModelRequestNode.run\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_did_stream:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# `self._result` gets set when exiting the `stream` contextmanager, so hitting this\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# means that the stream was started but not finished before `run()` was called\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mAgentRunError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou must finish streaming before calling run()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(ctx)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:304\u001b[0m, in \u001b[0;36mModelRequestNode._make_request\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    303\u001b[0m model_settings, model_request_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(ctx)\n\u001b[0;32m--> 304\u001b[0m model_response, request_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mdeps\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    305\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mmessage_history, model_settings, model_request_parameters\n\u001b[1;32m    306\u001b[0m )\n\u001b[1;32m    307\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39mincr(_usage\u001b[38;5;241m.\u001b[39mUsage(), requests\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finish_handling(ctx, model_response, request_usage)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic_ai/models/openai.py:197\u001b[0m, in \u001b[0;36mOpenAIModel.request\u001b[0;34m(self, messages, model_settings, model_request_parameters)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    192\u001b[0m     messages: \u001b[38;5;28mlist\u001b[39m[ModelMessage],\n\u001b[1;32m    193\u001b[0m     model_settings: ModelSettings \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    194\u001b[0m     model_request_parameters: ModelRequestParameters,\n\u001b[1;32m    195\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ModelResponse, usage\u001b[38;5;241m.\u001b[39mUsage]:\n\u001b[1;32m    196\u001b[0m     check_allow_model_requests()\n\u001b[0;32m--> 197\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_completions_create(\n\u001b[1;32m    198\u001b[0m         messages, \u001b[38;5;28;01mFalse\u001b[39;00m, cast(OpenAIModelSettings, model_settings \u001b[38;5;129;01mor\u001b[39;00m {}), model_request_parameters\n\u001b[1;32m    199\u001b[0m     )\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(response), _map_usage(response)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic_ai/models/openai.py:290\u001b[0m, in \u001b[0;36mOpenAIModel._completions_create\u001b[0;34m(self, messages, stream, model_settings, model_request_parameters)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APIStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (status_code \u001b[38;5;241m:=\u001b[39m e\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[0;32m--> 290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ModelHTTPError(status_code\u001b[38;5;241m=\u001b[39mstatus_code, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, body\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39mbody) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mModelHTTPError\u001b[0m: status_code: 400, model_name: dolphin-mixtral:8x7b, body: {'message': 'registry.ollama.ai/library/dolphin-mixtral:8x7b does not support tools', 'type': 'api_error', 'param': None, 'code': None}"
     ]
    }
   ],
   "source": [
    "class OutputFormat(pydantic.BaseModel):\n",
    "    answer: str = pydantic.Field(description='The answer to the user\\'s question.')\n",
    "    reasons: list[str] = pydantic.Field(description='The reasons for the answer.')\n",
    "\n",
    "result = await agent.run(\n",
    "    user_prompt=\"What is the capital of France?\",\n",
    "    result_type=OutputFormat,\n",
    ")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
