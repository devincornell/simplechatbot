{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Welcome to the <code>simplechatbot</code> Python package! This package provides tools for working with LLM agents - in particular, chatbots that track tools and conversation history.</p> <p>See the examples in the navbar to the left!</p>"},{"location":"#upcoming-features","title":"Upcoming Features","text":"<ul> <li>Adding clone methods.</li> <li>Update repr on <code>StreamResult</code>.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install git+ssh://git@github.com/devincornell/simplechatbot.git@main\n</code></pre> <p>When inside the package directory: Basic install: </p> <p><code>pip install .</code></p> <p>This package uses buildtools - see <code>pyproject.toml</code> for package details.</p>"},{"location":"#makefile","title":"Makefile","text":"<p>You can also use <code>make</code>.</p> <p>To install: </p> <pre><code>make install\nmake uninstall\n</code></pre>"},{"location":"#importing","title":"Importing","text":"<p>Basic importing works as you would expect.</p> <p><code>import simplechatbot</code></p>"},{"location":"#generating-documentation","title":"Generating Documentation","text":"<p>The Makefile has most of these commands, but including them here jsut in case.</p> <pre><code>pip install mkdocs\npip install mkdocs-material\n</code></pre> <p>Start Test Server</p> <pre><code>mkdocs serve\n</code></pre> <p>Build the documentation.</p> <pre><code>mkdocs build\n</code></pre> <p>Publish the documation</p> <pre><code>mkdocs gh-deploy --force\n</code></pre>"},{"location":"#example-documentation","title":"Example Documentation","text":"<p>In the Makefile I included the commands that will take example jupyter notebooks and convert them to markdown so that <code>mkdocs</code> can eventually convert them to html for the website. Simply add a notebook to the <code>site_examples</code> folder and it will be automatically converted to markdown and placed in the right folder.</p> <pre><code>EXAMPLE_NOTEBOOK_FOLDER = ./site_examples/# this is where example notebooks are stored\nEXAMPLE_NOTEBOOK_MARKDOWN_FOLDER = ./docs/examples/# this is where example notebooks are stored\n\nexample_notebooks:\n    -mkdir $(EXAMPLE_NOTEBOOK_MARKDOWN_FOLDER)\n    jupyter nbconvert --to markdown $(EXAMPLE_NOTEBOOK_FOLDER)/*.ipynb\n    mv $(EXAMPLE_NOTEBOOK_FOLDER)/*.md $(EXAMPLE_NOTEBOOK_MARKDOWN_FOLDER)\n</code></pre>"},{"location":"examples/a-overview/","title":"Introduction","text":"<p>This is a brief introduction to the <code>simplechatbot</code> package.</p> <pre><code>import sys\nsys.path.append('..')\n\nimport simplechatbot\n</code></pre>"},{"location":"examples/a-overview/#instantiating-chatbot-objects","title":"Instantiating <code>ChatBot</code> Objects","text":"<p><code>ChatBot</code> instances maintain three elements: a chat model (or runnable) LLM, chat history, and available tools / functions.</p> <p>It may be instantiated from any langchain chat model or runnable.</p> <pre><code>from langchain_openai import ChatOpenAI\n\n# optional: use this to grab keys from a json file rather than setting system variables\nkeychain = simplechatbot.APIKeyChain.from_json_file('../keys.json')\n\nopenai_model = ChatOpenAI(model='gpt-4o-mini', api_key=keychain['openai'])\nchatbot = simplechatbot.ChatBot.from_model(model=openai_model)\nprint(chatbot)\n</code></pre> <pre><code>ChatBot(model_type=ChatOpenAI, model_name=\"gpt-4o-mini\", tools=None)\n</code></pre> <p>The <code>tools</code> parameter allows you to pass any langchain tools you want your chatbot to be able to use. You can use one of Langchain's built-in tools (such as <code>FileManagementToolkit</code>) or define your own custom tools. I will use <code>FileManagementToolkit</code> for demonstration purposes here.</p> <pre><code>import langchain_core.tools\n\n@langchain_core.tools.tool\ndef check_new_messages(text: str, username: str) -&gt; str:\n    '''Check messages.'''\n    return f'No new messages.'\n\nchatbot = simplechatbot.ChatBot.from_model(\n    model = openai_model,\n    tools = [check_new_messages],\n)\n</code></pre> <p>You can see that tools are added to an internal <code>ToolSet</code> object.</p> <pre><code>chatbot.toolset\n</code></pre> <pre><code>ToolSet(tools={'check_new_messages': StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x12d08f600&gt;)})\n</code></pre> <p>Set a system prompt for the chatbot by passing it as the <code>system_prompt</code> argument.</p> <pre><code>system_prompt = '''\nYou are a creative designer who has been tasked with creating a new slogan for a company.\nThe user will describe the company, and you will need to generate three slogan ideas for them.\n'''\nchatbot = simplechatbot.ChatBot.from_model(\n    model = openai_model,\n    tools = [check_new_messages],\n    system_prompt=system_prompt,\n)\n</code></pre> <p>While the LLM itself is just a function, we build conversation-like behavior by storing a chat history. In <code>simplechatbot</code>, the history is stored in a <code>ChatHistory</code>, which is just a list subtype where list elements contain langchain <code>BaseMessage</code> subtypes. You can access it through the <code>history</code> property, and work with it just as a list.</p> <p>Here you can see that the system prompt is simply added as the first message in the chatbot history. </p> <pre><code>chatbot.history\n</code></pre> <pre><code>[SystemMessage(content='\\nYou are a creative designer who has been tasked with creating a new slogan for a company.\\nThe user will describe the company, and you will need to generate three slogan ideas for them.\\n', additional_kwargs={}, response_metadata={})]\n</code></pre> <p>To see the conversation history that is sent to the LLM, you can use the <code>get_buffer_string</code> method. This uses the same langchain methods used to invoke the LLM, so it is useful for debugging.</p> <pre><code>print(chatbot.history.get_buffer_string())\n</code></pre> <pre><code>System: \nYou are a creative designer who has been tasked with creating a new slogan for a company.\nThe user will describe the company, and you will need to generate three slogan ideas for them.\n</code></pre> <p>Note that history is a <code>list</code> subtype, so you can iterate through messages as you would expect.</p> <pre><code>for m in chatbot.history:\n    print(m)\n</code></pre> <pre><code>content='\\nYou are a creative designer who has been tasked with creating a new slogan for a company.\\nThe user will describe the company, and you will need to generate three slogan ideas for them.\\n' additional_kwargs={} response_metadata={}\n</code></pre>"},{"location":"examples/a-overview/#high-level-chat-and-chat_stream-methods","title":"High-level <code>chat</code> and <code>chat_stream</code> Methods","text":"<p>There are two primary methods used to interact with the chatbot: <code>chat</code> and <code>chat_stream</code>. </p> <p>These are the method use-cases:</p> <p><code>.chat()</code> \u2192 <code>ChatResult</code>: Use when you want to retrieve the full LLM response at once when it finishes.</p> <p><code>.chat_stream()</code> \u2192 <code>ChatStream</code>: Use when you would like to show intermediary results to the user as they are received from the LLM.</p> <pre><code>chatbot.chat('My name is Devin.')\n</code></pre> <pre><code>ChatResult(content=Hi Devin! How can I assist you today? Are you looking to create a slogan for a specific company or project?, tool_calls=[])\n</code></pre> <pre><code>chatbot.chat_stream('My name is Devin and I am a creative designer.')\n</code></pre> <pre><code>ChatStream(message_iter=&lt;generator object RunnableBindingBase.stream at 0x106f2d300&gt;, chatbot=ChatBot(model_type=ChatOpenAI, model_name=\"gpt-4o-mini\", tools=['check_new_messages']), toolset=ToolSet(tools={'check_new_messages': StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x12d08f600&gt;)}), add_reply_to_history=True, full_message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={}), exhausted=False)\n</code></pre> <p>Again use the <code>get_buffer_string</code> method to conveniently view the chat history.</p> <pre><code>print(chatbot.history.get_buffer_string())\n</code></pre> <pre><code>System: \nYou are a creative designer who has been tasked with creating a new slogan for a company.\nThe user will describe the company, and you will need to generate three slogan ideas for them.\n\nHuman: My name is Devin.\nAI: Hi Devin! How can I assist you today? Are you looking to create a slogan for a specific company or project?\nHuman: My name is Devin and I am a creative designer.\n</code></pre> <p>From the response to the prompt below you can see that it is maintained in the chat history because it \"retains\" knowledge that is given to it.</p> <pre><code>chatbot.chat('I have a quiz for you: what is my name?')\n</code></pre> <pre><code>ChatResult(content=Your name is Devin!, tool_calls=[])\n</code></pre>"},{"location":"examples/a-overview/#chat-and-chatresult-objects","title":"<code>.chat()</code> and <code>ChatResult</code> Objects","text":"<p>The <code>chat</code> method submits the current message and all history to the LLM and returns the reply as a <code>ChatResult</code> object.</p> <pre><code>chatbot.chat('Hello world.')\n</code></pre> <pre><code>ChatResult(content=Hello, Devin! How can I assist you today?, tool_calls=[])\n</code></pre> <p>If you want to submit the current chat history but do not want to add a new message, you can pass <code>None</code> as the message argument.</p> <pre><code>chatbot.chat(None)\n</code></pre> <pre><code>ChatResult(content=If there's anything specific you'd like to discuss or if you need help with a project, feel free to let me know!, tool_calls=[])\n</code></pre> <p>Alternatively, if you want to submit a query to the LLM but do not want to save it in the history, set <code>add_to_history = False</code>.</p> <pre><code>chatbot.chat('Hello world.', add_to_history=False)\n</code></pre> <pre><code>ChatResult(content=Hello again, Devin! How can I help you today?, tool_calls=[])\n</code></pre> <p><code>ChatResult</code> objects are returned from <code>chat()</code> and <code>invoke()</code> calls and include the LLM response text or tool calling information.</p> <pre><code>result = chatbot.chat('What is my name?')\nresult\n</code></pre> <pre><code>ChatResult(content=Your name is Devin!, tool_calls=[])\n</code></pre> <p>If no tool calls were requested from the LLM, you can access the response as a string through the <code>content</code> property.</p> <pre><code>result.content\n</code></pre> <pre><code>'Your name is Devin!'\n</code></pre> <p>If tool calls were made, the content will be empty but you can get information about any tool calls through the <code>tool_calls</code> attribute. Notice that no tool calls were requested by the LLM in the response to this query.</p> <pre><code>result.tool_calls\n</code></pre> <pre><code>[]\n</code></pre> <p>If there were tool calls, you can execute them using the <code>execute_tools</code> method.</p> <pre><code>result.execute_tools()\n</code></pre> <pre><code>{}\n</code></pre> <p>We provided the chatbot with a tool called <code>check_new_messages</code> earlier, and the LLM will request a tool call if the user requests it.</p> <pre><code>result = chatbot.chat('Check new messages.')\nresult.tool_calls\n</code></pre> <pre><code>[ToolCallInfo(id='call_zLXf8TVFZd3azJ88wuPZfB3S', name='check_new_messages', type='tool_call', args={'text': 'Check new messages.', 'username': 'devin'}, tool=StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x12d08f600&gt;))]\n</code></pre> <p>The <code>execute_tools</code> method returns a dictionary of <code>ToolCallResult</code> objects which contain the tool call information from the LLM (<code>ToolCallInfo</code>) and the return value of the tool execution.</p> <pre><code>tool_results = result.execute_tools()\ntool_results\n</code></pre> <pre><code>{'check_new_messages': ToolCallResult(info=ToolCallInfo(id='call_zLXf8TVFZd3azJ88wuPZfB3S', name='check_new_messages', type='tool_call', args={'text': 'Check new messages.', 'username': 'devin'}, tool=StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x12d08f600&gt;)), return_value='No new messages.')}\n</code></pre> <p>Use the <code>return_value</code> attribute to access these results.</p> <pre><code>tool_results['check_new_messages'].return_value\n</code></pre> <pre><code>'No new messages.'\n</code></pre>"},{"location":"examples/a-overview/#chat_stream-and-streamresult-objects","title":"<code>.chat_stream()</code> and <code>StreamResult</code> Objects","text":"<p><code>chat_stream</code> is very similar to <code>chat</code>, but allows you to return content to the user as soon as the LLM produces it. The method returns a <code>StreamResult</code> object which has an iterator interface that accumulates results from the LLM while also returning incremental results.</p> <p>In this example, I call <code>chat_stream</code> to retrieve a <code>StreamResult</code> object, which I then iterate through to retrieve and print all results.</p> <pre><code>stream = chatbot.chat_stream('What is my name?')\nfor r in stream:\n    print(r.content, end='', flush=True)\nstream\n</code></pre> <pre><code>Your name is Devin!\n\n\n\n\nChatStream(message_iter=&lt;generator object RunnableBindingBase.stream at 0x12ce61b70&gt;, chatbot=ChatBot(model_type=ChatOpenAI, model_name=\"gpt-4o-mini\", tools=['check_new_messages']), toolset=ToolSet(tools={'check_new_messages': StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x12d08f600&gt;)}), add_reply_to_history=True, full_message=AIMessageChunk(content='Your name is Devin!', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47'}), exhausted=True)\n</code></pre> <p>You can check the <code>exhausted</code> flag to see if the LLM has returned all results yet.</p> <pre><code>stream = chatbot.chat_stream('What is my name?')\nprint(stream.exhausted)\nfor r in stream:\n    print(r.content, end='', flush=True)\nprint(stream.exhausted)\nstream\n</code></pre> <pre><code>False\nYour name is Devin!True\n\n\n\n\n\nChatStream(message_iter=&lt;generator object RunnableBindingBase.stream at 0x12ce61f30&gt;, chatbot=ChatBot(model_type=ChatOpenAI, model_name=\"gpt-4o-mini\", tools=['check_new_messages']), toolset=ToolSet(tools={'check_new_messages': StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x12d08f600&gt;)}), add_reply_to_history=True, full_message=AIMessageChunk(content='Your name is Devin!', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47'}), exhausted=True)\n</code></pre> <p>After retrieving all of the LLM response, you can check if any tool calls are required.</p> <pre><code>stream = chatbot.chat_stream('Check my messages.')\nfor r in stream:\n    print(r.content, end='', flush=True)\nstream.tool_calls\n</code></pre> <pre><code>[ToolCallInfo(id='call_RONeUxpHmfeVKHg8AHJm1LyX', name='check_new_messages', type='tool_call', args={'text': 'Check my messages.', 'username': 'devin'}, tool=StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x12d08f600&gt;))]\n</code></pre> <p>And you would similarly execute tools by calling <code>execute_tools</code>. Note that you cannot call this method if the stream has not been exhausted.</p> <pre><code>stream.execute_tools()\n</code></pre> <pre><code>{'check_new_messages': ToolCallResult(info=ToolCallInfo(id='call_RONeUxpHmfeVKHg8AHJm1LyX', name='check_new_messages', type='tool_call', args={'text': 'Check my messages.', 'username': 'devin'}, tool=StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x12d08f600&gt;)), return_value='No new messages.')}\n</code></pre> <p>You can use the <code>result</code> method to get a <code>ChatResult</code> object instead. If it has not retrieved all results from the LLM, it will do so before returning.</p> <pre><code>chatbot.chat_stream('Hello world.').result()\n</code></pre> <pre><code>ChatResult(content=Hello again, Devin! How can I assist you today?, tool_calls=[])\n</code></pre>"},{"location":"examples/a-overview/#low-level-llm-methods-invoke-and-stream","title":"Low-level LLM Methods: <code>invoke</code> and <code>stream</code>","text":"<p>These lower-level <code>invoke</code> and <code>stream</code> methods are used by the <code>chat</code> and <code>chat_stream</code> methods to submit prompts to the LLM. They can allow you to interact with the LLM and tools/functions without chat history. Their signatures are very similar to high-level methods and they return the same types.</p> <p>NOTE: These methods ignore the system prompt!</p> <p>The low-level <code>invoke</code> method returns a <code>ChatResult</code> object with the content and tool call information.</p> <pre><code>result = chatbot.invoke('Hello world!')\nresult\n</code></pre> <pre><code>ChatResult(content=Hello! How can I assist you today?, tool_calls=[])\n</code></pre> <p>And <code>stream</code> is very similar to <code>chat_stream</code> except that it ignores chat history.</p> <pre><code>stream = chatbot.stream('Check messages.')\nfor r in stream:\n    print(r.content, end='', flush=True)\nstream.execute_tools()\n</code></pre> <pre><code>Please provide the text of the message and the username you want to check.\n\n\n\n\n{}\n</code></pre>"},{"location":"examples/a-overview/#chat-user-interface","title":"Chat User Interface","text":"<p>Of course, what is a chatbot if you can't actually use it? To run an interactive command-line chat, use <code>.ui.start_interactive</code>.</p> <pre><code># uncomment to start interactive chat\n#chatbot.ui.start_interactive(stream=True, show_intro=True, show_tools=True)\n</code></pre>"},{"location":"examples/b-tool_calling/","title":"Tool Calling","text":"<p><code>simplechatbot</code> empowers chatbot agents with the ability to produce arguments for arbitrary user functions instead of providing a text response to the user's prompt. Using this interface you can enable features such as web searching, email sending/checking, file browsing, image creation, or any other functionality that can be accessed through Python. The LLM will \"decide\" whether and which tools/functions should be executed based on a given prompt, so the key is to use tools with clear and concise instructions.</p> <p>Under the hood, <code>ChatBot</code> instances maintain a collection of langchain tools which can be extracted from toolkits or even factory methods that accept the chatbot itself as a parameter. Tools may also be added at the time of LLM execution to enable dynamic systems of available tools.</p> <p>You can create your own custom tools or choose from Langchain's built-in tools. I will use <code>FileManagementToolkit</code> for demonstration purposes here.</p> <pre><code>import sys\nsys.path.append('..')\n\nimport simplechatbot\nfrom simplechatbot.openai import OpenAIChatBot\n</code></pre>"},{"location":"examples/b-tool_calling/#enabling-tools","title":"Enabling Tools","text":"<p>Start by creating a new example tool that can enables the LLM to check email for the user. We create this tool using the <code>@langchain_core.tools.tool</code> decorator.</p> <pre><code>import langchain_core.tools\n\n@langchain_core.tools.tool\ndef check_new_messages() -&gt; str:\n    '''Check messages.'''\n    return f'No new messages.'\n</code></pre> <p>We include this tool as part of the chatbot by passing the function through the <code>tools</code> argument.</p> <pre><code>keychain = simplechatbot.APIKeyChain.from_json_file('../keys.json')\n\nsystem_prompt = '''\nYou are designed to answer any question the user has and send/check messages if needed.\nWhen the user requests you to check your messages, you should display the retrieved messages\n to the user.\n'''\n\nchatbot = OpenAIChatBot.new(\n    model_name = 'gpt-4o-mini', \n    api_key=keychain['openai'],\n    system_prompt=system_prompt,\n    tools = [check_new_messages],\n)\n</code></pre> <p>Now the LLM will have access to these tools. While the chatbot instance stores the LLM object in the <code>_model</code> attribute, you can use <code>model</code> to get the LLM with bound tools.</p> <pre><code>chatbot.model\n</code></pre> <pre><code>RunnableBinding(bound=ChatOpenAI(client=&lt;openai.resources.chat.completions.Completions object at 0x106927860&gt;, async_client=&lt;openai.resources.chat.completions.AsyncCompletions object at 0x12c19f7d0&gt;, root_client=&lt;openai.OpenAI object at 0x12bcd5a60&gt;, root_async_client=&lt;openai.AsyncOpenAI object at 0x12c19da90&gt;, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'check_new_messages', 'description': 'Check messages.', 'parameters': {'properties': {}, 'type': 'object'}}}]}, config={}, config_factories=[])\n</code></pre> <p>You can also use the method <code>get_model_with_tools</code> to get the tool-bound model with any additional tools. The <code>invoke</code>, <code>stream</code>, <code>chat</code>, and <code>chat_stream</code> methods all use this under-the hood so you can add any tools, toolkits, or tool factories to the model at invokation.</p> <pre><code>chatbot.get_model_with_tools(tools=None)\n</code></pre> <pre><code>(RunnableBinding(bound=ChatOpenAI(client=&lt;openai.resources.chat.completions.Completions object at 0x106927860&gt;, async_client=&lt;openai.resources.chat.completions.AsyncCompletions object at 0x12c19f7d0&gt;, root_client=&lt;openai.OpenAI object at 0x12bcd5a60&gt;, root_async_client=&lt;openai.AsyncOpenAI object at 0x12c19da90&gt;, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'check_new_messages', 'description': 'Check messages.', 'parameters': {'properties': {}, 'type': 'object'}}}]}, config={}, config_factories=[]),\n ToolSet(tools={'check_new_messages': StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x12c0dcea0&gt;)}))\n</code></pre> <p>Tools will be automatically used when we call any of the invoke or stream methods.</p> <p>Notice that the LLM behaves normally if the user's prompts are unrelated to the tool.</p> <pre><code>chatbot.invoke('Hello world!')\n</code></pre> <pre><code>ChatResult(content=Hello! How can I assist you today?, tool_calls=[])\n</code></pre> <p>If the LLM \"decides\" that the user needs to execute a tool, it returns a tool call as the response instead of returning content.</p> <pre><code>result = chatbot.invoke('Check my messages.')\nresult\n</code></pre> <pre><code>ChatResult(content=, tool_calls=[ToolCallInfo(id='call_ihfaGlFUvSl8OM0VQH4TkO9G', name='check_new_messages', type='tool_call', args={}, tool=StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x12c0dcea0&gt;))])\n</code></pre> <p>The tool call information can be accessed through the <code>ChatResult.tool_calls</code> attribute, which is simply a list supertype. Use <code>tool_info_str</code> to clearly show the arguments being passed to the function.</p> <pre><code>for tc in result.tool_calls:\n    print(tc.tool_info_str())\n</code></pre> <pre><code>check_new_messages()\n</code></pre> <p>You may also provide additional tools at the time of invoking the LLM, and it will be treated as if it was part of the chatbot. </p> <p>In this example, we create a new tool with two arguments that must be provided by the LLM.</p> <pre><code>@langchain_core.tools.tool\ndef send_message(recipient: str, text: str) -&gt; str:\n    '''Send messages to others.'''\n    return f'Message sent!'\n\nresult = chatbot.invoke('Send a message to Bob saying \"Hello!\"', tools=[send_message])\nresult\n</code></pre> <pre><code>ChatResult(content=, tool_calls=[ToolCallInfo(id='call_uXbyuViXTXHVO1zVohqydcW7', name='send_message', type='tool_call', args={'recipient': 'Bob', 'text': 'Hello!'}, tool=StructuredTool(name='send_message', description='Send messages to others.', args_schema=&lt;class 'langchain_core.utils.pydantic.send_message'&gt;, func=&lt;function send_message at 0x12c744720&gt;))])\n</code></pre> <p>You can see that the LLM provided the <code>recipient</code> and <code>text</code> arguments which were passed to the function call information.</p> <pre><code>result.tool_calls[0].tool_info_str()\n</code></pre> <pre><code>'send_message(recipient=Bob, text=Hello!)'\n</code></pre> <p>You can adjust behavior using the <code>tool_choice</code> argument in the chatbot constructor or at invokation. The value <code>'any'</code> means that a tool MUST be called, but all tools are candidates. The value <code>'auto'</code> (the default) allows the LLM to reply with normal content rather than a tool call, and you can also pass the name of a specific function as well.</p> <pre><code>result = chatbot.invoke('Go to the store for me!', tool_choice='any')\nresult.tool_calls[0].tool_info_str()\n</code></pre> <pre><code>'check_new_messages()'\n</code></pre>"},{"location":"examples/b-tool_calling/#executing-tools","title":"Executing Tools","text":"<p>Tools allow the LLM to determine if and when to execute tools and also provides parameters for the tool call based on conversation history, but the user containing function is responsible for actually executing the tool with the arguments from the LLM.</p> <p>Use the <code>execute_tools</code> method to actually execute the tool, which returns a mapping of tool names to <code>ToolCallResult</code> objects.</p> <pre><code>result = chatbot.invoke('Check my messages.')\nresult.tool_calls[0].tool_info_str()\n</code></pre> <pre><code>'check_new_messages()'\n</code></pre> <pre><code>tr = result.execute_tools()\ntr\n</code></pre> <pre><code>{'check_new_messages': ToolCallResult(info=ToolCallInfo(id='call_2KihzJGsgJ2Y0Qxy6r7920x6', name='check_new_messages', type='tool_call', args={}, tool=StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x12c0dcea0&gt;)), return_value='No new messages.')}\n</code></pre> <p>Get the return value from the tool through the <code>return_value</code> property.</p> <pre><code>tr['check_new_messages'].return_value\n</code></pre> <pre><code>'No new messages.'\n</code></pre> <p>Extracting tool calls from a <code>StreamResult</code> is a little more complicated because the stream must be exhausted before executing tools. This happens because the tool call information replaces the text response, so the streamer is essentially receiving chunks of the function call information until exhaustion.</p> <p>The calling function must handle both the streamed output and tool calls.</p> <pre><code>stream = chatbot.stream('Check my messages.')\nfor r in stream:\n    print(r, end='', flush=True)\nif len(stream.tool_calls) &gt; 0:\n    stream.execute_tools()\n</code></pre> <pre><code>content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_Fq2o50ejL69YZjZugGxRrycv', 'function': {'arguments': '', 'name': 'check_new_messages'}, 'type': 'function'}]} response_metadata={} id='run-18230eaa-b7ab-4104-a231-eb30688ab393' tool_calls=[{'name': 'check_new_messages', 'args': {}, 'id': 'call_Fq2o50ejL69YZjZugGxRrycv', 'type': 'tool_call'}] tool_call_chunks=[{'name': 'check_new_messages', 'args': '', 'id': 'call_Fq2o50ejL69YZjZugGxRrycv', 'index': 0, 'type': 'tool_call_chunk'}]content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '{}', 'name': None}, 'type': None}]} response_metadata={} id='run-18230eaa-b7ab-4104-a231-eb30688ab393' tool_calls=[{'name': '', 'args': {}, 'id': None, 'type': 'tool_call'}] tool_call_chunks=[{'name': None, 'args': '{}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]content='' additional_kwargs={} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b'} id='run-18230eaa-b7ab-4104-a231-eb30688ab393'\n</code></pre> <p>A <code>ValueError</code> will be raised if the caller tries to execute tools before the stream is exhausted.</p> <pre><code>stream = chatbot.stream('Check my messages.')\nfor r in stream:\n    print(r.content, end='', flush=True)\n    break\ntry:\n    stream.execute_tools()\nexcept ValueError as e:\n    print('Exception was caught!')\n</code></pre> <pre><code>Exception was caught!\n</code></pre>"},{"location":"examples/b-tool_calling/#toolkits-and-tool-factories","title":"Toolkits and Tool Factories","text":"<p>Aside from providing a list of tools, you may also bind tools from toolkits and tool factories.</p> <ul> <li> <p><code>ToolKit</code>: class with a <code>get_tools() -&gt; list[BaseTool]</code> method. <code>ToolKit</code>s are part of the langchain interface, and the built-in tools often come as a subtype. Passed through the <code>toolkits: list[BaseToolkit]</code> argument.</p> </li> <li> <p>Tool Factories: functions that accept a chatbot as an argument and return tools. Useful when writing tools that interact with the original LLM because otherwise it would require partial initialization. Passed through the <code>tool_factories: ToolFactoryType</code> argument.</p> </li> </ul> <p>Note that these too may be provided at instantiation or at invokation.</p>"},{"location":"examples/b-tool_calling/#toolkit-example","title":"<code>ToolKit</code> Example","text":"<p>In this example, I enable the built-in Langchain <code>FileManagementToolkit</code> toolkit to allow the chatbot to list, read, and write files.</p> <pre><code>import tempfile\nfrom langchain_community.agent_toolkits import FileManagementToolkit\nwith tempfile.TemporaryDirectory() as wd:\n    file_tk = FileManagementToolkit(root_dir=str(wd))\n    result = chatbot.invoke('List the files in this directory.', toolkits=[file_tk])\n    print(result.tool_calls[0].tool_info_str())\n</code></pre> <pre><code>list_directory()\n</code></pre>"},{"location":"examples/b-tool_calling/#tool-factory-examples","title":"Tool Factory Examples","text":"<p>Now I create a tool factory that can be passed to the chatbot. This tool uses the chatbot reference to invoke the LLM with access to all of the same tools.</p> <pre><code>\ndef my_tool_factory(chatbot: simplechatbot.ChatBot) -&gt; list[langchain_core.tools.Tool]:\n    @langchain_core.tools.tool\n    def story_generator(topic: str) -&gt; str:\n        '''Generate a story absed on a particular topic.'''\n        result = chatbot.invoke(\n            f'Generate a story about {topic}. Your response should only include the text of the story and make it short but engaging.',\n        )\n        return result.content\n\n    return [story_generator]\n\nresult = chatbot.invoke('Generate a story about western cowboys.', tool_factories=[my_tool_factory])\ntc_result = result.execute_tools()\ntc_result['story_generator'].return_value\n</code></pre> <pre><code>'In the heart of the Wild West, where the sun kissed the rugged plains, a small town named Dusty Gulch thrived amidst the dust and the grit. The townsfolk revered their sheriff, a grizzled cowboy named Hank \"Six-Shooter\" McGraw, known for his quick draw and even quicker wit.\\n\\nOne fateful afternoon, a notorious outlaw gang, led by the infamous Red Jack, rode into town, stirring trouble and fear. With a band of rowdy gunslingers, they demanded the townspeople hand over their gold or face the consequences. The saloon doors swung open, and the townsfolk watched, hearts pounding, as Hank stepped out, his spurs jingling like a warning bell.\\n\\n\u201cNow hold on there, Red Jack,\u201d Hank called, his voice steady as the mountains. \u201cYou can\u2019t just waltz into Dusty Gulch and expect to take what ain\\'t yours.\u201d\\n\\nRed Jack smirked, his hand twitching above his holster. \u201cYou think you can stop us, McGraw? You\u2019re just one man against my crew.\u201d\\n\\nWith a confident grin, Hank replied, \u201cOne man with a little courage can make a mighty stand.\u201d The tension crackled like lightning in the summer air.\\n\\nAs the sun began to set, casting long shadows across the dirt street, a showdown loomed. Hank\u2019s hand hovered over his revolver, eyes locked on Red Jack. The town held its breath.\\n\\nIn a flash of movement, gunfire erupted, echoing through the canyon like thunder. Dust flew, and the air was thick with the smell of gunpowder. One by one, Hank outmaneuvered the outlaws, his sharpshooting a testament to years of practice. With a final, decisive shot, Red Jack crumpled to the ground.\\n\\nSilence fell over Dusty Gulch. The townspeople erupted in cheers as Hank stood tall, the hero of the day. They surrounded him, gratitude shining in their eyes, as he tipped his hat. In that moment, the spirit of the West thrived, a reminder that courage and honor could still prevail in a world of chaos.'\n</code></pre>"},{"location":"examples/b-tool_calling/#conclusions","title":"Conclusions","text":"<p>That is all! Now you know how to enable and disable tools that your LLM can use to do anything!</p>"},{"location":"examples/c-model_specific_chatbots/","title":"Model-specific ChatBots","text":"<p>While <code>ChatBot</code> instances can be created from any Langchain Chat interface, we created some convenient superclasses that have varying levels of model-specific behavior.</p> <p>Model-specific chatbots only differ from <code>ChatBot</code> in that they define static factory constructor methods, all named <code>new</code>. As each chat model needs to be installed separately, they must be accessed via separate imports.</p> <pre><code>import sys\nsys.path.append('..')\n\nimport simplechatbot\n</code></pre> <p>I will use the keychain to manage API keys for OpenAI and Mistral.</p> <pre><code>keychain = simplechatbot.devin.APIKeyChain.from_json_file('../keys.json')\n</code></pre> <p>Notice that we use a separate import statement to explicitly import the model-specific chatbots.</p> <pre><code>from simplechatbot.devin.ollama import OllamaChatBot\n\nchatbot = OllamaChatBot.new(\n    model_name = 'llama3.1', \n)\n</code></pre> <pre><code>from simplechatbot.devin.openai import OpenAIChatBot\n\nchatbot = OpenAIChatBot.new(\n    model_name = 'gpt-4o-mini', \n    api_key=keychain['openai'],\n)\n</code></pre> <pre><code>from simplechatbot.devin.mistral import MistralChatBot\n\nchatbot = MistralChatBot.new(\n    model_name = 'mistral-large-latest', \n    api_key=keychain['mistral'],\n)\n</code></pre>"}]}