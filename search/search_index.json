{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Welcome to the <code>simplechatbot</code> Python package! This package provides tools for working with LLM agents - in particular, chatbots that track tools and conversation history.</p> <p>See the examples in the navigation bar to the left!</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install git+ssh://git@github.com/devincornell/simplechatbot.git@main\n</code></pre> <p>When inside the package directory: Basic install: </p> <p><code>pip install .</code></p> <p>This package uses buildtools - see <code>pyproject.toml</code> for package details.</p>"},{"location":"#makefile","title":"Makefile","text":"<p>You can also use <code>make</code>.</p> <p>To install: </p> <pre><code>make install\nmake reinstall\nmake uninstall\n</code></pre>"},{"location":"#importing","title":"Importing","text":"<p>Basic importing works as you would expect.</p> <p><code>import simplechatbot</code></p>"},{"location":"#generating-documentation","title":"Generating Documentation","text":"<p>The Makefile has most of these commands, but including them here jsut in case.</p> <pre><code>pip install mkdocs\npip install mkdocs-material\n</code></pre> <p>Start Test Server</p> <pre><code>mkdocs serve\n</code></pre> <p>Build the documentation.</p> <pre><code>mkdocs build\n</code></pre> <p>Publish the documation</p> <pre><code>mkdocs gh-deploy --force\n</code></pre>"},{"location":"#example-documentation","title":"Example Documentation","text":"<p>In the Makefile I included the commands that will take example jupyter notebooks and convert them to markdown so that <code>mkdocs</code> can eventually convert them to html for the website. Simply add a notebook to the <code>site_examples</code> folder and it will be automatically converted to markdown and placed in the right folder.</p> <pre><code>EXAMPLE_NOTEBOOK_FOLDER = ./site_examples/# this is where example notebooks are stored\nEXAMPLE_NOTEBOOK_MARKDOWN_FOLDER = ./docs/examples/# this is where example notebooks are stored\n\nexample_notebooks:\n    -mkdir $(EXAMPLE_NOTEBOOK_MARKDOWN_FOLDER)\n    python -m pymddoc ipynb2md-multi $(EXAMPLE_NOTEBOOK_FOLDER)/*.ipynb\n    mv $(EXAMPLE_NOTEBOOK_FOLDER)/*.md $(EXAMPLE_NOTEBOOK_MARKDOWN_FOLDER)\n</code></pre>"},{"location":"examples/a-overview/","title":"API Introduction","text":"<p>This is a brief introduction to the <code>simplechatbot</code> API.</p> <pre><code>import sys\nsys.path.append('../src/')\n\nimport simplechatbot\n</code></pre>"},{"location":"examples/a-overview/#instantiating-agent-objects","title":"Instantiating <code>Agent</code> Objects","text":"<p><code>Agent</code> instances maintain three elements: a chat model (or runnable) LLM, chat history, and available tools / functions. System prompts are simply stored as part of the conversation history.</p> <p>It may be instantiated from any langchain chat model or runnable.</p> <pre><code>from langchain_openai import ChatOpenAI\n\n# optional: use this to grab keys from a json file rather than setting system variables\nkeychain = simplechatbot.APIKeyChain.from_json_file('../keys.json')\n\nopenai_model = ChatOpenAI(model='gpt-4o-mini', api_key=keychain['openai'])\nagent = simplechatbot.Agent.from_model(model=openai_model)\nprint(agent)\n</code></pre> <p>stdout:</p> <pre><code>Agent(model_type=ChatOpenAI, model_name=\"gpt-4o-mini\", tools=ToolLookup(tools={}))\n</code></pre> <p>Set a system prompt for the agent by passing it as the <code>system_prompt</code> argument.</p> <pre><code>system_prompt = '''\nYou are a creative designer who has been tasked with creating a new slogan for a company.\nThe user will describe the company, and you will need to generate three slogan ideas for them.\n'''\nagent = simplechatbot.Agent.from_model(\n    model = openai_model,\n    system_prompt=system_prompt,\n)\n</code></pre> <p>The <code>tools</code> parameter allows you to pass any langchain tools you want your agent to be able to use. You can use one of Langchain's built-in tools (such as <code>FileManagementToolkit</code>) or define your own custom tools. I will use <code>FileManagementToolkit</code> for demonstration purposes here.</p> <pre><code>import langchain_core.tools\n\n@langchain_core.tools.tool\ndef check_new_messages(text: str, username: str) -&gt; str:\n    '''Check messages.'''\n    return f'No new messages.'\n\nagent = simplechatbot.Agent.from_model(\n    model = openai_model,\n    tools = [check_new_messages],\n)\n</code></pre> <p>You can see that tools are added to an internal <code>ToolSet</code> object.</p> <pre><code>agent.toolset\n</code></pre> <p>text:</p> <pre><code>ToolSet(tools={'check_new_messages': StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x131f511c0&gt;)}, tool_factories=[], tool_choice=None)\n</code></pre> <p>While the LLM itself is just a function, we build conversation-like behavior by storing a chat history. In <code>simplechatbot</code>, the history is stored in a <code>ChatHistory</code>, which is just a list subtype where list elements contain langchain <code>BaseMessage</code> subtypes. You can access it through the <code>history</code> property, and work with it just as a list.</p> <p>Here you can see that the system prompt is simply added as the first message in the agent history. </p> <pre><code>agent.history\n</code></pre> <p>text:</p> <pre><code>[]\n</code></pre> <p>To see the conversation history that is sent to the LLM, you can use the <code>get_buffer_string</code> method. This uses the same langchain methods used to invoke the LLM, so it is useful for debugging.</p> <pre><code>print(agent.history.get_buffer_string())\n</code></pre> <p>stdout:</p> <p>Note that history is a <code>list</code> subtype, so you can iterate through messages as you would expect.</p> <pre><code>for m in agent.history:\n    print(m)\n</code></pre>"},{"location":"examples/a-overview/#high-level-chat-and-stream-methods","title":"High-level <code>chat</code> and <code>stream</code> Methods","text":"<p>There are two primary methods used to interact with the chatbot: <code>chat</code> and <code>stream</code>. </p> <p>These are the method use-cases:</p> <p><code>.chat()</code> \u2192 <code>ChatResult</code>: Use when you want to retrieve the full LLM response at once when it finishes.</p> <p><code>.stream()</code> \u2192 <code>ChatStream</code>: Use when you would like to show intermediary results to the user as they are received from the LLM.</p> <pre><code>agent.chat('My name is Devin.')\n</code></pre> <p>text:</p> <pre><code>ChatResult(content=Nice to meet you, Devin! How can I assist you today?, tool_calls=[])\n</code></pre> <pre><code>agent.stream('My name is Devin and I am a creative designer.')\n</code></pre> <p>text:</p> <pre><code>StreamResult(message_iter=&lt;generator object RunnableBindingBase.stream at 0x131542d40&gt;, agent=Agent(model_type=ChatOpenAI, model_name=\"gpt-4o-mini\", tools=ToolLookup(tools={'check_new_messages': StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x131f511c0&gt;)})), tool_lookup=ToolLookup(tools={'check_new_messages': StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x131f511c0&gt;)}), add_reply_to_history=True, full_message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={}), exhausted=False, receive_callback=None)\n</code></pre> <p>Again use the <code>get_buffer_string</code> method to conveniently view the chat history.</p> <pre><code>print(agent.history.get_buffer_string())\n</code></pre> <p>stdout:</p> <pre><code>Human: My name is Devin.\nAI: Nice to meet you, Devin! How can I assist you today?\nHuman: My name is Devin and I am a creative designer.\n</code></pre> <p>From the response to the prompt below you can see that it is maintained in the chat history because it \"retains\" knowledge that is given to it.</p> <pre><code>agent.chat('I have a quiz for you: what is my name?')\n</code></pre> <p>text:</p> <pre><code>ChatResult(content=Your name is Devin!, tool_calls=[])\n</code></pre>"},{"location":"examples/a-overview/#chat-and-chatresult-objects","title":"<code>.chat()</code> and <code>ChatResult</code> Objects","text":"<p>The <code>chat</code> method submits the current message and all history to the LLM and returns the reply as a <code>ChatResult</code> object.</p> <pre><code>agent.chat('Hello world.')\n</code></pre> <p>text:</p> <pre><code>ChatResult(content=Hello, Devin! How are you today?, tool_calls=[])\n</code></pre> <p>If you want to submit the current chat history but do not want to add a new message, you can pass <code>None</code> as the message argument.</p> <pre><code>agent.chat(None)\n</code></pre> <p>text:</p> <pre><code>ChatResult(content=Is there anything specific you'd like to discuss or any questions you have?, tool_calls=[])\n</code></pre> <p>Alternatively, if you want to submit a query to the LLM but do not want to save it in the history, set <code>add_to_history = False</code>.</p> <pre><code>agent.chat('Hello world.', add_to_history=False)\n</code></pre> <p>text:</p> <pre><code>ChatResult(content=Hello again, Devin! If there's anything specific you'd like to talk about or if you have any questions, feel free to share!, tool_calls=[])\n</code></pre> <p><code>ChatResult</code> objects are returned from <code>chat()</code> and <code>invoke()</code> calls and include the LLM response text or tool calling information.</p> <pre><code>result = agent.chat('What is my name?')\nresult\n</code></pre> <p>text:</p> <pre><code>ChatResult(content=Your name is Devin., tool_calls=[])\n</code></pre> <p>If no tool calls were requested from the LLM, you can access the response as a string through the <code>content</code> property.</p> <pre><code>result.content\n</code></pre> <p>text:</p> <pre><code>'Your name is Devin.'\n</code></pre> <p>If tool calls were made, the content will be empty but you can get information about any tool calls through the <code>tool_calls</code> attribute. Notice that no tool calls were requested by the LLM in the response to this query.</p> <pre><code>result.tool_calls\n</code></pre> <p>text:</p> <pre><code>[]\n</code></pre> <p>If there were tool calls, you can execute them using the <code>execute_tools</code> method.</p> <pre><code>result.execute_tools()\n</code></pre> <p>text:</p> <pre><code>{}\n</code></pre> <p>We provided the agent with a tool called <code>check_new_messages</code> earlier, and the LLM will request a tool call if the user requests it.</p> <pre><code>result = agent.chat('Check new messages.')\nresult.tool_calls\n</code></pre> <p>text:</p> <pre><code>[ToolCallInfo(id='call_208aNIO1I1TlXxijnOdWYQFf', name='check_new_messages', type='tool_call', args={'text': 'Check new messages.', 'username': 'Devin'}, tool=StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x131f511c0&gt;))]\n</code></pre> <p>The <code>execute_tools</code> method returns a dictionary of <code>ToolCallResult</code> objects which contain the tool call information from the LLM (<code>ToolCallInfo</code>) and the return value of the tool execution.</p> <pre><code>tool_results = result.execute_tools()\ntool_results\n</code></pre> <p>text:</p> <pre><code>{'check_new_messages': ToolCallResult(info=ToolCallInfo(id='call_208aNIO1I1TlXxijnOdWYQFf', name='check_new_messages', type='tool_call', args={'text': 'Check new messages.', 'username': 'Devin'}, tool=StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x131f511c0&gt;)), return_value='No new messages.')}\n</code></pre> <p>Use the <code>return_value</code> attribute to access these results.</p> <pre><code>tool_results['check_new_messages'].return_value\n</code></pre> <p>text:</p> <pre><code>'No new messages.'\n</code></pre>"},{"location":"examples/a-overview/#stream-and-streamresult-objects","title":"<code>.stream()</code> and <code>StreamResult</code> Objects","text":"<p><code>stream</code> is very similar to <code>chat</code>, but allows you to return content to the user as soon as the LLM produces it. The method returns a <code>StreamResult</code> object which has an iterator interface that accumulates results from the LLM while also returning incremental results.</p> <p>In this example, I call <code>stream</code> to retrieve a <code>StreamResult</code> object, which I then iterate through to retrieve and print all results.</p> <pre><code>stream = agent.stream('What is my name?')\nfor r in stream:\n    print(r.content, end='', flush=True)\nstream\n</code></pre> <p>stdout:</p> <pre><code>Your name is Devin.\n</code></pre> <p>text:</p> <pre><code>StreamResult(message_iter=&lt;generator object RunnableBindingBase.stream at 0x1229117b0&gt;, agent=Agent(model_type=ChatOpenAI, model_name=\"gpt-4o-mini\", tools=ToolLookup(tools={'check_new_messages': StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x131f511c0&gt;)})), tool_lookup=ToolLookup(tools={'check_new_messages': StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x131f511c0&gt;)}), add_reply_to_history=True, full_message=AIMessageChunk(content='Your name is Devin.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306'}), exhausted=True, receive_callback=None)\n</code></pre> <p>You can check the <code>exhausted</code> flag to see if the LLM has returned all results yet.</p> <pre><code>stream = agent.stream('What is my name?')\nprint(stream.exhausted)\nfor r in stream:\n    print(r.content, end='', flush=True)\nprint(stream.exhausted)\nstream\n</code></pre> <p>stdout:</p> <pre><code>False\nYour name is Devin.True\n</code></pre> <p>text:</p> <pre><code>StreamResult(message_iter=&lt;generator object RunnableBindingBase.stream at 0x1315422f0&gt;, agent=Agent(model_type=ChatOpenAI, model_name=\"gpt-4o-mini\", tools=ToolLookup(tools={'check_new_messages': StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x131f511c0&gt;)})), tool_lookup=ToolLookup(tools={'check_new_messages': StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x131f511c0&gt;)}), add_reply_to_history=True, full_message=AIMessageChunk(content='Your name is Devin.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306'}), exhausted=True, receive_callback=None)\n</code></pre> <p>After retrieving all of the LLM response, you can check if any tool calls are required.</p> <pre><code>stream = agent.stream('Check my messages.')\nfor r in stream:\n    print(r.content, end='', flush=True)\nstream.tool_calls\n</code></pre> <p>text:</p> <pre><code>[ToolCallInfo(id='call_wxhHcOtuj5XPSHvnbFVvOXrQ', name='check_new_messages', type='tool_call', args={'text': 'Check my messages.', 'username': 'Devin'}, tool=StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x131f511c0&gt;))]\n</code></pre> <p>And you would similarly execute tools by calling <code>execute_tools</code>. Note that you cannot call this method if the stream has not been exhausted.</p> <pre><code>stream.execute_tools()\n</code></pre> <p>text:</p> <pre><code>{'check_new_messages': ToolCallResult(info=ToolCallInfo(id='call_wxhHcOtuj5XPSHvnbFVvOXrQ', name='check_new_messages', type='tool_call', args={'text': 'Check my messages.', 'username': 'Devin'}, tool=StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x131f511c0&gt;)), return_value='No new messages.')}\n</code></pre> <p>You can use the <code>result</code> method to get a <code>ChatResult</code> object instead. If it has not retrieved all results from the LLM, it will do so before returning.</p> <pre><code>agent.stream('Hello world.').collect()\n</code></pre> <p>text:</p> <pre><code>ChatResult(content=Hello again, Devin! What would you like to talk about?, tool_calls=[])\n</code></pre>"},{"location":"examples/a-overview/#low-level-llm-methods-_invoke-and-_stream","title":"Low-level LLM Methods: <code>_invoke</code> and <code>_stream</code>","text":"<p>These lower-level <code>_invoke</code> and <code>_stream</code> methods are used by the <code>chat</code> and <code>chat_stream</code> methods to submit prompts to the LLM. They can allow you to interact with the LLM and tools/functions without chat history. Their signatures are very similar to high-level methods and they return the same types.</p> <p>NOTE: These methods ignore the system prompt!</p> <p>The low-level <code>_invoke</code> method returns a <code>ChatResult</code> object with the content and tool call information.</p> <pre><code>result = agent._invoke('Hello world!')\nresult\n</code></pre> <p>text:</p> <pre><code>ChatResult(content=Hello! How can I assist you today?, tool_calls=[])\n</code></pre> <p>And <code>stream</code> is very similar to <code>stream</code> except that it ignores chat history.</p> <pre><code>stream = agent._stream('Check messages.')\nfor r in stream:\n    print(r.content, end='', flush=True)\nstream.execute_tools()\n</code></pre> <p>stdout:</p> <pre><code>Can you please provide your username and the message you would like to check?\n</code></pre> <p>text:</p> <pre><code>{}\n</code></pre>"},{"location":"examples/a-overview/#chat-user-interface","title":"Chat User Interface","text":"<p>Of course, what is a chatbot if you can't actually use it? To run an interactive command-line chat, use <code>.ui.start_interactive</code>.</p> <pre><code># uncomment to start interactive chat\n#agent.ui.start_interactive(stream=True, show_intro=True, show_tools=True)\n</code></pre>"},{"location":"examples/b-model_specific_chatbots/","title":"Model-specific ChatBots","text":"<p>While <code>ChatBot</code> instances can be created from any Langchain Chat interface, we created some convenient superclasses that have varying levels of model-specific behavior.</p> <p>Model-specific chatbots only differ from <code>ChatBot</code> in that they define static factory constructor methods, all named <code>new</code>. As each chat model needs to be installed separately, they must be accessed via separate imports.</p> <pre><code>import sys\nsys.path.append('../src/')\n\nimport simplechatbot\n</code></pre> <p>I will use the keychain to manage API keys for OpenAI and Mistral.</p> <pre><code>keychain = simplechatbot.APIKeyChain.from_json_file('../keys.json')\n</code></pre> <p>Notice that we use a separate import statement to explicitly import the model-specific chatbots.</p> <pre><code>from simplechatbot.ollama_agent import OllamaAgent\n\nagent = OllamaAgent.new(\n    model_name = 'llama3.1', \n)\n</code></pre> <pre><code>from simplechatbot.openai_agent import OpenAIAgent\n\nagent = OpenAIAgent.new(\n    model_name = 'gpt-4o-mini', \n    api_key=keychain['openai'],\n)\n</code></pre> <pre><code>from simplechatbot.mistral_agent import MistralAgent\n\nagent = MistralAgent.new(\n    model_name = 'mistral-large-latest', \n    api_key=keychain['mistral'],\n)\n</code></pre>"},{"location":"examples/c-structured_outputs/","title":"Structured Outputs","text":"<p>One of the most powerful features of LLMs is the ability to produce outputs which conform to a pre-determined structure. There are several instances in which this feature is critical.</p> <ul> <li>Output structures may enforce \"reasoning\" or systematic approaches to generating responses.</li> <li>The output must be passed to an application or interface that is not simply text-based.</li> </ul> <p>We define output structures using custom Pydantic types with attribute descriptions and validation logics. See more about using Pydantic types for structured outputs using langchain here. This package builds on that functionality by making it easy to request structured output at any point in an exchange.</p> <pre><code>import pydantic\n\nimport sys\nsys.path.append('../src/')\n\nimport simplechatbot\nfrom simplechatbot.openai_agent import OpenAIAgent\n</code></pre> <p>Create a new basic agent with no system prompt and no tools.</p> <pre><code># optional: use this to grab keys from a json file rather than setting system variables\nkeychain = simplechatbot.APIKeyChain.from_json_file('../keys.json')\n\nagent = OpenAIAgent.new('gpt-4o-mini', api_key=keychain['openai'])\nprint(agent)\n</code></pre> <p>stdout:</p> <pre><code>OpenAIAgent(model_type=ChatOpenAI, model_name=\"gpt-4o-mini\", tools=ToolLookup(tools={}))\n</code></pre> <p>Note that this is a normal agent that can be conversed with.</p> <pre><code>agent.stream('Hello, how are you? My name is Devin. Don\\'t forget it!').print_and_collect()\n</code></pre> <p>stdout:</p> <pre><code>Hello, Devin! I'm doing well, thank you. How can I assist you today?\n</code></pre> <p>text:</p> <pre><code>ChatResult(content=Hello, Devin! I'm doing well, thank you. How can I assist you today?, tool_calls=[])\n</code></pre>"},{"location":"examples/c-structured_outputs/#pydantic-types-and-chat_structured","title":"Pydantic Types and <code>chat_structured</code>","text":"<p>The <code>chat_structured</code> method allows you to provide an output structure that the LLM response will be constrained to. The example below forces the LLM to answer the question and even come up with a follow-up question.</p> <pre><code>class ResponseFormatter(pydantic.BaseModel):\n    \"\"\"Always use this tool to structure your response to the user.\"\"\"\n    answer: str = pydantic.Field(description=\"The answer to the user's question.\")\n    followup_question: str = pydantic.Field(description=\"A follow-up question the user could ask.\")\n\nsresult = agent.chat_structured('what is your favorite cat?', output_structure=ResponseFormatter)\nsresult\n</code></pre> <p>text:</p> <pre><code>StructuredOutputResult(data=answer=\"As an AI, I don't have personal preferences or feelings, but popular cat breeds that many people love include the Maine Coon for their friendly nature and size, the Siamese for their vocal personality, and the Ragdoll for their affectionate demeanor. What about you, Devin? Do you have a favorite cat?\" followup_question='What qualities do you look for in a favorite cat?')\n</code></pre> <p>As you can see, the <code>chat_structured</code> method returns a <code>StructuredOutputResult</code> instance, which has a <code>data</code> attribute which actually stores the Pydantic type instance.</p> <pre><code>sresult.data\n</code></pre> <p>text:</p> <pre><code>ResponseFormatter(answer=\"As an AI, I don't have personal preferences or feelings, but popular cat breeds that many people love include the Maine Coon for their friendly nature and size, the Siamese for their vocal personality, and the Ragdoll for their affectionate demeanor. What about you, Devin? Do you have a favorite cat?\", followup_question='What qualities do you look for in a favorite cat?')\n</code></pre> <p>Access the attributes of the response through this instance.</p> <pre><code>sresult.data.answer\n</code></pre> <p>text:</p> <pre><code>\"As an AI, I don't have personal preferences or feelings, but popular cat breeds that many people love include the Maine Coon for their friendly nature and size, the Siamese for their vocal personality, and the Ragdoll for their affectionate demeanor. What about you, Devin? Do you have a favorite cat?\"\n</code></pre> <pre><code>sresult.data.followup_question\n</code></pre> <p>text:</p> <pre><code>'What qualities do you look for in a favorite cat?'\n</code></pre> <p>You can see this object in json format using the <code>as_json</code> method.</p> <pre><code>print(sresult.as_json(indent=2))\n</code></pre> <p>stdout:</p> <pre><code>{\n  \"answer\": \"As an AI, I don't have personal preferences or feelings, but popular cat breeds that many people love include the Maine Coon for their friendly nature and size, the Siamese for their vocal personality, and the Ragdoll for their affectionate demeanor. What about you, Devin? Do you have a favorite cat?\",\n  \"followup_question\": \"What qualities do you look for in a favorite cat?\"\n}\n</code></pre> <p>You can see that the response was included in the conversation history in json format. Fortunately, modern LLMs can easily parse json data structures to keep track of conversation progress.</p> <pre><code>print(agent.history.get_buffer_string())\n</code></pre> <p>stdout:</p> <pre><code>Human: Hello, how are you? My name is Devin. Don't forget it!\nAI: Hello, Devin! I'm doing well, thank you. How can I assist you today?\nHuman: what is your favorite cat?\nAI: {\"answer\":\"As an AI, I don't have personal preferences or feelings, but popular cat breeds that many people love include the Maine Coon for their friendly nature and size, the Siamese for their vocal personality, and the Ragdoll for their affectionate demeanor. What about you, Devin? Do you have a favorite cat?\",\"followup_question\":\"What qualities do you look for in a favorite cat?\"}\n</code></pre> <p>Asking it again by passing <code>new_message=None</code> shows that it will simply ask the questions in plain text format, so it will re-use the structured response provided in the previous message.</p> <pre><code>agent.stream(None).print_and_collect()\n</code></pre> <p>stdout:</p> <pre><code>As an AI, I don't have personal preferences or feelings, but popular cat breeds that many people love include the Maine Coon for their friendly nature and size, the Siamese for their vocal personality, and the Ragdoll for their affectionate demeanor. What about you, Devin? Do you have a favorite cat?\n</code></pre> <p>text:</p> <pre><code>ChatResult(content=As an AI, I don't have personal preferences or feelings, but popular cat breeds that many people love include the Maine Coon for their friendly nature and size, the Siamese for their vocal personality, and the Ragdoll for their affectionate demeanor. What about you, Devin? Do you have a favorite cat?, tool_calls=[])\n</code></pre>"},{"location":"examples/c-structured_outputs/#complex-structures-and-response-order","title":"Complex Structures and Response Order","text":"<p>The most powerful aspect of structured responses is that they force the LLM to provide parts of the full response separately and in-sequence. Carefully designed output structures can lead to better and more complete responses.</p> <p>In the following questions, we ask the LLM to provide a response to the question \"Why are cheetahs so fast?\" with different output structures.</p> <pre><code>class ReasonedResponse(pydantic.BaseModel):\n    \"\"\"Always use this tool to structure your response to the user.\"\"\"\n    answer: str = pydantic.Field(description=\"The answer to the user's question.\")\n    reasons: list[str] = pydantic.Field(description=\"Reasons for the answer.\")\n\nsresult = agent.chat_structured('Why are cheetahs so fast?', output_structure=ReasonedResponse, add_to_history=False)\nprint(sresult.as_json(indent=2))\n</code></pre> <p>stdout:</p> <pre><code>{\n  \"answer\": \"Cheetahs are fast due to their unique physical adaptations and evolutionary traits.\",\n  \"reasons\": [\n    \"Cheetahs have a lightweight body structure, which reduces drag when running.\",\n    \"They possess long, powerful legs that enable rapid acceleration and speed.\",\n    \"Their flexible spine allows for an extended stride length, increasing their speed.\",\n    \"Cheetahs have large nasal passages and lungs that facilitate increased oxygen intake during high-speed chases.\"\n  ]\n}\n</code></pre> <pre><code>class ReasonedResponse2(pydantic.BaseModel):\n    \"\"\"This is a well-reasoned response.\"\"\"\n    reasons: list[str] = pydantic.Field(description=\"Reasons for the answer.\")\n    answer: str = pydantic.Field(description=\"The answer to the user's question.\")\n\nsresult = agent.chat_structured('Why are cheetahs so fast?', output_structure=ReasonedResponse2, add_to_history=False)\nprint(sresult.as_json(indent=2))\n</code></pre> <p>stdout:</p> <pre><code>{\n  \"reasons\": [\n    \"Cheetahs have a lightweight body structure and long legs designed for speed.\",\n    \"Their flexible spine allows for an extended stride length while running.\",\n    \"Muscle composition in cheetahs is optimized for quick bursts of speed, containing a high percentage of fast-twitch muscle fibers.\",\n    \"They have large nasal passages for increased oxygen intake and a specialized respiratory system that facilitates rapid breathing during sprints.\"\n  ],\n  \"answer\": \"Cheetahs are so fast due to their lightweight body, long leg structure, flexible spine, and muscle composition optimized for speed.\"\n}\n</code></pre> <p>You may also create nested response structures. In this case, we further improve reasoning abilities by requiring the LLM to self-rate the quality of its own responses, leading to a final answer which places special emphasis on these reasons.</p> <pre><code>class SingleReason(pydantic.BaseModel):\n    \"\"\"This is a single reason and a self-assessment of the quality of the reason.\"\"\"\n    reason: str = pydantic.Field(description=\"A description of the reason.\")\n    quality: int = pydantic.Field(description=\"Quality score represented by an integer between 1 and 10.\")\n\nclass ReasonedResponse3(pydantic.BaseModel):\n    \"\"\"This is a well-reasoned response.\"\"\"\n    reasons: list[SingleReason] = pydantic.Field(description=\"Reasons for the answer.\")\n    answer: str = pydantic.Field(description=\"The answer to the user's question, based on the given reasons and emphsaizing the highest quality reasons.\")\n\nsresult = agent.chat_structured('Why are cheetahs so fast?', output_structure=ReasonedResponse3, add_to_history=False)\nprint(sresult.as_json(indent=2))\n</code></pre> <p>stdout:</p> <pre><code>{\n  \"reasons\": [\n    {\n      \"reason\": \"Cheetahs have a lightweight body structure that reduces drag and allows for quicker acceleration.\",\n      \"quality\": 9\n    },\n    {\n      \"reason\": \"Their leg muscles are highly specialized for sprinting, providing powerful and rapid movement.\",\n      \"quality\": 8\n    },\n    {\n      \"reason\": \"Cheetahs possess large nostrils that allow for increased oxygen intake during a sprint, supporting their high-speed chases.\",\n      \"quality\": 7\n    },\n    {\n      \"reason\": \"Their flexible spine enables a longer stride length while running, enhancing speed.\",\n      \"quality\": 8\n    }\n  ],\n  \"answer\": \"Cheetahs are so fast due to their lightweight body structure, specialized leg muscles, large nostrils for oxygen intake, and a flexible spine that allows for longer strides.\"\n}\n</code></pre> <p>Different approaches for output structures and ordering can lead to vastly different results, so, as with most Generative AI applications, experimentation is essential!</p>"},{"location":"examples/d-tool_calling/","title":"Tool Calling","text":"<p><code>simplechatbot</code> empowers agents with the ability to produce arguments for arbitrary user functions instead of providing a text response to the user's prompt. Using this interface you can enable features such as web searching, email sending/checking, file browsing, image creation, or any other functionality that can be accessed through Python. The LLM will \"decide\" whether and which tools/functions should be executed based on a given prompt, so the key is to use tools with clear and concise instructions.</p> <p>Under the hood, <code>ChatBot</code> instances maintain a collection of langchain tools which can be extracted from toolkits or even factory methods that accept the chatbot itself as a parameter. Tools may also be added at the time of LLM execution to enable dynamic systems of available tools.</p> <p>You can create your own custom tools or choose from Langchain's built-in tools. I will use <code>FileManagementToolkit</code> for demonstration purposes here.</p> <pre><code>import sys\nsys.path.append('../src/')\n\nimport simplechatbot\nfrom simplechatbot.openai_agent import OpenAIAgent\n</code></pre>"},{"location":"examples/d-tool_calling/#enabling-tools","title":"Enabling Tools","text":"<p>Start by creating a new example tool that can enables the LLM to check email for the user. We create this tool using the <code>@langchain_core.tools.tool</code> decorator.</p> <pre><code>import langchain_core.tools\n\n@langchain_core.tools.tool\ndef check_new_messages() -&gt; str:\n    '''Check messages.'''\n    return f'No new messages.'\n</code></pre> <p>We include this tool as part of the chatbot by passing the function through the <code>tools</code> argument.</p> <pre><code>keychain = simplechatbot.APIKeyChain.from_json_file('../keys.json')\n\nsystem_prompt = '''\nYou are designed to answer any question the user has and send/check messages if needed.\nWhen the user requests you to check your messages, you should display the retrieved messages\n to the user.\n'''\n\nagent = OpenAIAgent.new(\n    model_name = 'gpt-4o-mini', \n    api_key=keychain['openai'],\n    system_prompt=system_prompt,\n    tools = [check_new_messages],\n)\n</code></pre> <p>Now the LLM will have access to these tools. While the agent instance stores the LLM object in the <code>_model</code> attribute, you can use <code>model</code> to get the LLM with bound tools.</p> <pre><code>agent.model\n</code></pre> <p>text:</p> <pre><code>RunnableBinding(bound=ChatOpenAI(client=&lt;openai.resources.chat.completions.completions.Completions object at 0x10cd3c080&gt;, async_client=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at 0x10cd3dd90&gt;, root_client=&lt;openai.OpenAI object at 0x10844b6e0&gt;, root_async_client=&lt;openai.AsyncOpenAI object at 0x10cd3c0e0&gt;, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'check_new_messages', 'description': 'Check messages.', 'parameters': {'properties': {}, 'type': 'object'}}}]}, config={}, config_factories=[])\n</code></pre> <p>You can also use the method <code>get_model_with_tools</code> to get the tool-bound model with any additional tools. The <code>invoke</code>, <code>stream</code>, <code>chat</code>, and <code>chat_stream</code> methods all use this under-the hood so you can add any tools, toolkits, or tool factories to the model at invokation.</p> <pre><code>agent.get_model_with_tools(tools=None)\n</code></pre> <p>text:</p> <pre><code>(RunnableBinding(bound=ChatOpenAI(client=&lt;openai.resources.chat.completions.completions.Completions object at 0x10cd3c080&gt;, async_client=&lt;openai.resources.chat.completions.completions.AsyncCompletions object at 0x10cd3dd90&gt;, root_client=&lt;openai.OpenAI object at 0x10844b6e0&gt;, root_async_client=&lt;openai.AsyncOpenAI object at 0x10cd3c0e0&gt;, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'check_new_messages', 'description': 'Check messages.', 'parameters': {'properties': {}, 'type': 'object'}}}]}, config={}, config_factories=[]),\n ToolLookup(tools={'check_new_messages': StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x1084516c0&gt;)}))\n</code></pre> <p>Tools will be automatically used when we call any of the invoke or stream methods.</p> <p>Notice that the LLM behaves normally if the user's prompts are unrelated to the tool.</p> <pre><code>agent._invoke('Hello world!')\n</code></pre> <p>text:</p> <pre><code>ChatResult(content=Hello! How can I assist you today?, tool_calls=[])\n</code></pre> <p>If the LLM \"decides\" that the user needs to execute a tool, it returns a tool call as the response instead of returning content.</p> <pre><code>result = agent._invoke('Check my messages.')\nresult\n</code></pre> <p>text:</p> <pre><code>ChatResult(content=, tool_calls=[ToolCallInfo(id='call_D0WyCNzn8Hg0qRxsui24aZ33', name='check_new_messages', type='tool_call', args={}, tool=StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x1084516c0&gt;))])\n</code></pre> <p>The tool call information can be accessed through the <code>ChatResult.tool_calls</code> attribute, which is simply a list supertype. Use <code>tool_info_str</code> to clearly show the arguments being passed to the function.</p> <pre><code>for tc in result.tool_calls:\n    print(tc.tool_info_str())\n</code></pre> <p>stdout:</p> <pre><code>check_new_messages()\n</code></pre> <p>You may also provide additional tools at the time of invoking the LLM, and it will be treated as if it was part of the chatbot. </p> <p>In this example, we create a new tool with two arguments that must be provided by the LLM.</p> <pre><code>@langchain_core.tools.tool\ndef send_message(recipient: str, text: str) -&gt; str:\n    '''Send messages to others.'''\n    return f'Message sent!'\n\nresult = agent._invoke('Send a message to Bob saying \"Hello!\"', tools=[send_message])\nresult\n</code></pre> <p>text:</p> <pre><code>ChatResult(content=, tool_calls=[ToolCallInfo(id='call_CaZgudgmby8bQahiyL2PnP6J', name='send_message', type='tool_call', args={'recipient': 'Bob', 'text': 'Hello!'}, tool=StructuredTool(name='send_message', description='Send messages to others.', args_schema=&lt;class 'langchain_core.utils.pydantic.send_message'&gt;, func=&lt;function send_message at 0x10cd45a80&gt;))])\n</code></pre> <p>You can see that the LLM provided the <code>recipient</code> and <code>text</code> arguments which were passed to the function call information.</p> <pre><code>result.tool_calls[0].tool_info_str()\n</code></pre> <p>text:</p> <pre><code>'send_message(recipient=Bob, text=Hello!)'\n</code></pre> <p>You can adjust behavior using the <code>tool_choice</code> argument in the chatbot constructor or at invokation. The value <code>'any'</code> means that a tool MUST be called, but all tools are candidates. The value <code>'auto'</code> (the default) allows the LLM to reply with normal content rather than a tool call, and you can also pass the name of a specific function as well.</p> <pre><code>result = agent._invoke('Go to the store for me!', tool_choice='any')\nresult.tool_calls[0].tool_info_str()\n</code></pre> <p>text:</p> <pre><code>'check_new_messages()'\n</code></pre>"},{"location":"examples/d-tool_calling/#executing-tools","title":"Executing Tools","text":"<p>Tools allow the LLM to determine if and when to execute tools and also provides parameters for the tool call based on conversation history, but the user containing function is responsible for actually executing the tool with the arguments from the LLM.</p> <p>Use the <code>execute_tools</code> method to actually execute the tool, which returns a mapping of tool names to <code>ToolCallResult</code> objects.</p> <pre><code>result = agent._invoke('Check my messages.')\nresult.tool_calls[0].tool_info_str()\n</code></pre> <p>text:</p> <pre><code>'check_new_messages()'\n</code></pre> <pre><code>tr = result.execute_tools()\ntr\n</code></pre> <p>text:</p> <pre><code>{'check_new_messages': ToolCallResult(info=ToolCallInfo(id='call_CPyijbPQpk91FIdl0B8NOLmI', name='check_new_messages', type='tool_call', args={}, tool=StructuredTool(name='check_new_messages', description='Check messages.', args_schema=&lt;class 'langchain_core.utils.pydantic.check_new_messages'&gt;, func=&lt;function check_new_messages at 0x1084516c0&gt;)), return_value='No new messages.')}\n</code></pre> <p>Get the return value from the tool through the <code>return_value</code> property.</p> <pre><code>tr['check_new_messages'].return_value\n</code></pre> <p>text:</p> <pre><code>'No new messages.'\n</code></pre> <p>Extracting tool calls from a <code>StreamResult</code> is a little more complicated because the stream must be exhausted before executing tools. This happens because the tool call information replaces the text response, so the streamer is essentially receiving chunks of the function call information until exhaustion.</p> <p>The calling function must handle both the streamed output and tool calls.</p> <pre><code>stream = agent._stream('Check my messages.')\nfor r in stream:\n    print(r, end='', flush=True)\nif len(stream.tool_calls) &gt; 0:\n    stream.execute_tools()\n</code></pre> <p>stdout:</p> <pre><code>content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_0I3oV4MeoavXZe8gyJygHkQZ', 'function': {'arguments': '', 'name': 'check_new_messages'}, 'type': 'function'}]} response_metadata={} id='run-029ed68d-e9ad-4a1f-9716-960302d7c64b' tool_calls=[{'name': 'check_new_messages', 'args': {}, 'id': 'call_0I3oV4MeoavXZe8gyJygHkQZ', 'type': 'tool_call'}] tool_call_chunks=[{'name': 'check_new_messages', 'args': '', 'id': 'call_0I3oV4MeoavXZe8gyJygHkQZ', 'index': 0, 'type': 'tool_call_chunk'}]content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': None, 'function': {'arguments': '{}', 'name': None}, 'type': None}]} response_metadata={} id='run-029ed68d-e9ad-4a1f-9716-960302d7c64b' tool_calls=[{'name': '', 'args': {}, 'id': None, 'type': 'tool_call'}] tool_call_chunks=[{'name': None, 'args': '{}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]content='' additional_kwargs={} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306'} id='run-029ed68d-e9ad-4a1f-9716-960302d7c64b'\n</code></pre> <p>A <code>ValueError</code> will be raised if the caller tries to execute tools before the stream is exhausted.</p> <pre><code>stream = agent.stream('Check my messages.')\nfor r in stream:\n    print(r.content, end='', flush=True)\n    break\ntry:\n    stream.execute_tools()\nexcept ValueError as e:\n    print('Exception was caught!')\n</code></pre> <p>stdout:</p> <pre><code>Exception was caught!\n</code></pre>"},{"location":"examples/d-tool_calling/#toolkits-and-tool-factories","title":"Toolkits and Tool Factories","text":"<p>Aside from providing a list of tools, you may also bind tools from toolkits and tool factories.</p> <ul> <li> <p><code>ToolKit</code>: class with a <code>get_tools() -&gt; list[BaseTool]</code> method. <code>ToolKit</code>s are part of the langchain interface, and the built-in tools often come as a subtype. Passed through the <code>toolkits: list[BaseToolkit]</code> argument.</p> </li> <li> <p>Tool Factories: functions that accept a chatbot as an argument and return tools. Useful when writing tools that interact with the original LLM because otherwise it would require partial initialization. Passed through the <code>tool_factories: ToolFactoryType</code> argument.</p> </li> </ul> <p>Note that these too may be provided at instantiation or at invokation.</p>"},{"location":"examples/d-tool_calling/#toolkit-example","title":"<code>ToolKit</code> Example","text":"<p>In this example, I enable the built-in Langchain <code>FileManagementToolkit</code> toolkit to allow the chatbot to list, read, and write files.</p> <pre><code>import tempfile\nfrom langchain_community.agent_toolkits import FileManagementToolkit\nwith tempfile.TemporaryDirectory() as wd:\n    file_tk = FileManagementToolkit(root_dir=str(wd))\n    result = agent._invoke('List the files in this directory.', toolkits=[file_tk])\n    print(result.tool_calls[0].tool_info_str())\n</code></pre> <p>stdout:</p> <pre><code>list_directory()\n</code></pre>"},{"location":"examples/d-tool_calling/#tool-factory-examples","title":"Tool Factory Examples","text":"<p>Now I create a tool factory that can be passed to the chatbot. This tool uses the chatbot reference to invoke the LLM with access to all of the same tools.</p> <pre><code>def my_tool_factory(agent: simplechatbot.Agent) -&gt; list[langchain_core.tools.Tool]:\n    @langchain_core.tools.tool\n    def story_generator(topic: str) -&gt; str:\n        '''Generate a story absed on a particular topic.'''\n        result = agent._invoke(\n            f'Generate a story about {topic}. Your response should only include the text of the story and make it short but engaging.',\n        )\n        return result.content\n\n    return [story_generator]\n\nresult = agent._invoke('Generate a story about western cowboys.', tool_factories=[my_tool_factory])\ntc_result = result.execute_tools()\ntc_result['story_generator'].return_value\n</code></pre> <p>text:</p> <pre><code>'In the heart of the rugged Wild West, a small town called Dusty Springs thrived under the watchful eyes of the surrounding mesas. The sun blazed overhead, casting long shadows over the dusty streets where only a handful of souls dared to tread. Among them was Colt Harper, a seasoned cowboy known for his swift draw and uncanny ability to ride like the wind.\\n\\nOne sweltering afternoon, a mysterious stranger rode into town on a sleek black stallion. Cloaked in a dust-covered duster and a wide-brimmed hat that obscured his face, he dismounted at the saloon, causing the townsfolk to whisper in hushed tones. Colt, nursing a whiskey at the bar, felt a stirring in his gut; trouble had a way of finding him.\\n\\nThe stranger, known only as Jake, challenged Colt to a duel at high noon. Rumors swirled that Jake was looking for revenge against Colt\u2019s brother, who had long since met his fate in a gunfight gone wrong. As the clock ticked towards noon, the townspeople gathered, tension crackling in the air like a summer storm.\\n\\nColt strode into the street, his boots kicking up dust as he faced Jake under the blazing sun. The world around them faded away, each heartbeat echoing in the silence. With a quick draw, Colt aimed and fired, the bullet whistling through the air and finding its mark. Jake fell, surprise blossoming on his face, and the whispering crowd gasped.\\n\\nAs the dust settled, Colt walked over to him. \u201cThis ain\u2019t no way to settle a score,\u201d he said, his voice steady. \u201cLet\u2019s put the past to rest. Life\u2019s too short.\u201d Jake nodded slowly, his anger replaced by a flicker of understanding.\\n\\nThe two men stood, embodying a spirit of camaraderie that echoed through the rugged landscape. Dusty Springs would remember that day not just for the gunfight, but for the moment a cowboy chose peace over vengeance. As the sun dipped below the horizon, Colt tipped his hat and rode off into the golden glow, a true gunslinger of the West.'\n</code></pre>"},{"location":"examples/d-tool_calling/#conclusions","title":"Conclusions","text":"<p>That is all! Now you know how to enable and disable tools that your LLM can use to do anything!</p>"},{"location":"examples/f-multi-agent_example/","title":"Multi-agent Examples","text":"<p>In this example I show an example of an agent that writes a story using a series of steps.</p> <ol> <li>The user provides an idea for their story.</li> <li><code>OutlineBot</code> generates an outline for the story including a topic, narrative arc, title, and individual chapter narratives, events, and titles.</li> <li><code>StoryBot</code> creates the first chapter based on the chatper and overall story outines.</li> <li><code>SummaryBot</code> creates a summary of the newly generated chapter.</li> <li>The chapter summary is passed to <code>StoryBot</code> along with section the outline to generate the next chapter.</li> <li>All chapter follow steps 3-5. ...</li> <li>The chapter are combined into a full story.</li> </ol> <p></p> <pre><code>import pydantic\nimport typing\nimport pathlib\n\nimport sys\nsys.path.append('../src/')\n\nimport simplechatbot\nfrom simplechatbot.openai_agent import OpenAIAgent\nfrom simplechatbot.ollama_agent import OllamaAgent\n</code></pre> <p>First I create a new agent from OpenAI using the API stored in the keychain file. Most importantly, the agent maintains a LangChain chat model so we can use the <code>base_agent</code> to create new application-specific agents.</p> <pre><code>if True:\n    keychain = simplechatbot.APIKeyChain.from_json_file('../keys.json')\n    base_agent = OpenAIAgent.new(\n        model_name = 'gpt-4o-mini', \n        api_key=keychain['openai'],\n    )\nelse:\n    base_agent = OllamaAgent.new(\n        model_name = 'llama3.1', \n    )\n</code></pre> <p>Next we create the <code>StoryOutline</code> pydantic type to describe the structure of the output. Notice that the <code>StoryOutline</code> has a variable number of chapters, each with its own narrative, events, and title. Also note that the attribute order for <code>StoryOutline</code> follows the flow of topic, narrative, chapters, and then finally title. The values will be generated in that order, so we choose an ordering that most effectively can capture the process of producing creative stories.</p> <p>Also essential to this agent is the system prompt, which can be adjusted given feedback about outline qualities.</p> <pre><code>class ChapterOutline(pydantic.BaseModel):\n    \"\"\"Outline of a chapter in a story.\"\"\"\n    narrative_arc: str = pydantic.Field(description=\"The narrative arc of the story.\")\n    events: typing.List[str] = pydantic.Field(description=\"List of the most important events that happen in the chapter.\")\n    title: str = pydantic.Field(description=\"Title of the chapter.\")\n\n\nclass StoryOutline(pydantic.BaseModel):\n    \"\"\"Outline of the story.\"\"\"\n    topic: str = pydantic.Field(description=\"The main topic of the story.\")\n    narrative_arc: str = pydantic.Field(description=\"The narrative arc of the story.\")\n    chapters: list[ChapterOutline] = pydantic.Field(description=\"List of chapters in the story.\")\n    title: str = pydantic.Field(description=\"Title of the story.\")\n\nclass OutlineBot:\n    system_prompt = (\n        \"The user will provide you a description of a story, and you must create a chapter outline with titles and brief descriptions. \"\n        \"Each section should contain a title and a brief description of what happens in that section. \"\n        \"The sections should all be part of a single narrative ark, but each section should have a complete beginning, middle, and end. \"\n    )\n    def __init__(self, base_agent: simplechatbot.Agent):\n        self.agent = base_agent.new_agent_from_model(\n            system_prompt=self.system_prompt,\n        )\n\n    def create_outline(self, story_description: str) -&gt; simplechatbot.StructuredOutputResult:\n        return self.agent.chat_structured(story_description, output_structure=StoryOutline)\n\noutline_bot = OutlineBot(base_agent)\noutline_bot\n</code></pre> <p>text:</p> <pre><code>&lt;__main__.OutlineBot at 0x109b68770&gt;\n</code></pre> <p>Next we create <code>ChapterBot</code>, which will generate the actual chapter content based on information generated in the outline and a summary of the previous chapter (if one exists).</p> <pre><code>class ChapterBot:\n    system_prompt = (\n        \"You are designed to write a single chapter of a larger story based on the following information: \\n\\n\"\n        \"+ Overall story topic: The topic of the full story.\\n\"\n        \"+ Chapter title: The title of the section you are writing.\\n\"\n        \"+ Chapter description: A longer description of what happens in the section.\\n\"\n        \"+ (optional) previous chapter summary: A summary of the previous chapter.\\n\"\n        \"Your responses should only include text that is part of the story. Do not include the chapter title \\n\"\n        \"or any other information that is not part of the story itself.\\n\"\n        \"The story chapter should be super short, so keep that in mind!\"\n    )\n\n    def __init__(self, base_agent: simplechatbot.Agent):\n        self.agent = base_agent.new_agent_from_model(\n            system_prompt=self.system_prompt,\n        )\n\n    def write_chapter(\n        self,\n        story_outline: StoryOutline,\n        chapter_outline: ChapterOutline,\n        prev_chapter_summary: typing.Optional[str] = None,\n    ) -&gt; str:\n        prompt = (\n            f'General story topic: \"{story_outline.topic}\"\\n\\n'\n            f'Chapter title: \"{chapter_outline.title}\"\\n\\n'\n            f'Chapter narrative arc: \"{chapter_outline.narrative_arc}\"\\n\\n'\n            f'Previous chapter summary: \"{prev_chapter_summary if prev_chapter_summary is not None else \"No previous chapter - this is the first!\"}\"'\n        )\n        return self.agent.stream(prompt, add_to_history=False).progress_and_collect().content\n\nchapter_bot = ChapterBot(base_agent)\n</code></pre> <p>Finally we create <code>SummaryBot</code>, which has the simple task of generating a summary of a given chapter.</p> <pre><code>class SummaryBot:\n    system_prompt = (\n        'You need to create a summary of the story chapter provided to you by the user. '\n        'The summary should include names of relevant characters and capture the story arc of the chapter. '\n    )\n\n    def __init__(self, base_agent: simplechatbot.Agent):\n        self.agent = base_agent.new_agent_from_model(\n            system_prompt=self.system_prompt,\n        )\n\n    def summarize(self, chapter_text: str) -&gt; str:\n        prompt = (\n            f'Chapter text:\\n\\n{chapter_text}'\n        )\n        return self.agent.stream(prompt, add_to_history=False).progress_and_collect().content\n\nsummary_bot = SummaryBot(base_agent)\n</code></pre> <p>Now we can create a <code>Story</code> type to tie it all together: it will contain the overal narrative arc as well as individual chapter outline, text, and summaries. The <code>generate_from_topic</code> method accepts a topic and from there will generate an outline and sequentially generate chapters and chapter summaries of the story.</p> <pre><code>import tqdm\n\nclass Chapter(pydantic.BaseModel):\n    i: int\n    outline: ChapterOutline\n    text: str\n    summary: str\n\nclass Story(pydantic.BaseModel):\n    outline: StoryOutline\n    chapters: typing.List[Chapter] = pydantic.Field(default_factory=list)\n\n    @classmethod\n    def generate_from_topic(\n        cls,\n        topic: str,\n        outline_bot: OutlineBot,\n        chapter_bot: ChapterBot,\n        summary_bot: SummaryBot,\n    ) -&gt; typing.Self:\n        outline_result = outline_bot.create_outline(topic)\n        outline: StoryOutline = outline_result.data\n\n        chapters = list()\n        prev_summary = None\n        for i, chapter in enumerate(tqdm.tqdm(outline.chapters, ncols=80)):\n            chapter_text = chapter_bot.write_chapter(\n                story_outline=outline, \n                chapter_outline=chapter,\n                prev_chapter_summary=prev_summary,\n            )\n            summary = summary_bot.summarize(chapter_text)\n            prev_summary = summary\n            chapters.append(Chapter(i=i, outline=chapter, text=chapter_text, summary=summary))\n        return cls(outline=outline, chapters=chapters)\n\n    def add_chapter(\n        self, \n        outline: ChapterOutline,\n        text: str,\n        summary: str,\n    ) -&gt; None:\n        self.chapters.append(Chapter(outline=outline, text=text, summary=summary))\n</code></pre> <p>Finally, we define a topic and generate a new story!</p> <pre><code>q = (\n    f'The story should be about two friends who met when they were young and then lost touch.'\n    'They meet again as adults and have to navigate their new relationship. '\n)\n\nstory = Story.generate_from_topic(\n    topic=q,\n    outline_bot=outline_bot,\n    chapter_bot=chapter_bot,\n    summary_bot=summary_bot,\n)\n</code></pre> <p>stderr:</p> <pre><code>342it [00:05, 63.03it/s]\n188it [00:02, 81.52it/s]\n257it [00:03, 72.37it/s]\n176it [00:02, 78.58it/s]\n439it [00:05, 79.36it/s]\n128it [00:01, 82.71it/s] \n355it [00:04, 82.32it/s]\n185it [00:04, 38.40it/s]\n457it [00:05, 80.11it/s]\n202it [00:02, 78.42it/s]\n355it [00:04, 81.00it/s]\n163it [00:01, 85.35it/s] \n6it [00:44,  7.39s/it]\n</code></pre> <pre><code>story.outline.title\n</code></pre> <p>text:</p> <pre><code>'Bridges to the Past'\n</code></pre> <pre><code>print(story.outline.narrative_arc)\n</code></pre> <p>stdout:</p> <pre><code>Two childhood friends, separated by life changes, reunite as adults and must grapple with their evolution while rekindling their bond.\n</code></pre> <pre><code>dest = pathlib.Path('story_results/')\ndest.mkdir(parents=True, exist_ok=True)\n\noverview = f'''{story.outline.title}\n{len(story.outline.chapters)} Chapters\n\nTopic: {story.outline.topic}\n\n--------------------------------\nNarrative:\n{story.outline.narrative_arc}\n--------------------------------\nChapters:\n\n'''\noverview += '\\n\\n'.join([f'{c.title}\\n{c.narrative_arc}' for c in story.outline.chapters])\nwith dest.joinpath('overview.txt').open('w') as f:\n    f.write(overview)\n\nfor chapter in story.chapters:\n    events = '\\n'.join([f'\\t{i+1}. {e}' for i, e in enumerate(chapter.outline.events)])\n    with dest.joinpath(f'{chapter.i}. {chapter.outline.title}.txt').open('w') as f:\n        f.write(f'{chapter.outline.title}\\n\\nNarrative: {chapter.outline.narrative_arc}\\n\\nEvents:\\n{events}\\n\\n------------------\\nSummary:\\n{chapter.summary}\\n\\n\\n------------------\\n{chapter.text}\\n\\n')\n</code></pre>"}]}