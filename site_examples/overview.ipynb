{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Brief overview of using ```simplechatbot``` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import simplechatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ChatBot` Objects\n",
    "\n",
    "`ChatBot` instances maintain three elements: a chat model (or runnable) LLM, chat history, and available tools / functions.\n",
    "\n",
    "It may be instantiated from any [langchain chat model](https://python.langchain.com/v0.1/docs/modules/model_io/chat/) or runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot(model_type=ChatOpenAI, model_name=\"gpt-4o-mini\", tools=None)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# optional: use this to grab keys from a json file rather than setting system variables\n",
    "keychain = simplechatbot.devin.APIKeyChain.from_json_file('../keys.json')\n",
    "\n",
    "openai_model = ChatOpenAI(model='gpt-4o-mini', api_key=keychain['openai'])\n",
    "chatbot = simplechatbot.devin.ChatBot.from_model(model=openai_model)\n",
    "print(chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting System Prompt\n",
    "Use the `system_prompt` parameter to initialize the chatbot with instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "You are a creative designer who has been tasked with creating a new slogan for a company.\n",
    "The user will describe the company, and you will need to generate three slogan ideas for them.\n",
    "'''\n",
    "chatbot = simplechatbot.devin.ChatBot.from_model(\n",
    "    model = openai_model,\n",
    "    system_prompt=system_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Tools\n",
    "The `tools` parameter allows you to pass any [langchain tools](https://python.langchain.com/v0.1/docs/modules/tools/) you want your chatbot to be able to use. You can use one of [Langchain's built-in tools](https://python.langchain.com/v0.1/docs/integrations/tools/) (such as `FileManagementToolkit`) or [define your own custom tools](https://python.langchain.com/v0.1/docs/modules/tools/custom_tools/). I will use `FileManagementToolkit` for demonstration purposes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CopyFileTool(root_dir='/var/folders/cx/g8dnkjjn3f3dy8r8_8kr12kc0000gn/T/tmp0jm9c49m'),\n",
       " DeleteFileTool(root_dir='/var/folders/cx/g8dnkjjn3f3dy8r8_8kr12kc0000gn/T/tmp0jm9c49m'),\n",
       " FileSearchTool(root_dir='/var/folders/cx/g8dnkjjn3f3dy8r8_8kr12kc0000gn/T/tmp0jm9c49m'),\n",
       " MoveFileTool(root_dir='/var/folders/cx/g8dnkjjn3f3dy8r8_8kr12kc0000gn/T/tmp0jm9c49m'),\n",
       " ReadFileTool(root_dir='/var/folders/cx/g8dnkjjn3f3dy8r8_8kr12kc0000gn/T/tmp0jm9c49m'),\n",
       " WriteFileTool(root_dir='/var/folders/cx/g8dnkjjn3f3dy8r8_8kr12kc0000gn/T/tmp0jm9c49m'),\n",
       " ListDirectoryTool(root_dir='/var/folders/cx/g8dnkjjn3f3dy8r8_8kr12kc0000gn/T/tmp0jm9c49m')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.agent_toolkits import FileManagementToolkit\n",
    "import tempfile #python standard library\n",
    "working_directory = tempfile.TemporaryDirectory()\n",
    "toolkit = FileManagementToolkit(\n",
    "    root_dir=str(working_directory.name)\n",
    ")\n",
    "tools = toolkit.get_tools()\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the tools to `ChatBot` will call `model.bind_tools(tools)` and then create a dictionary of tools that is referenced when attempting to execute a tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = simplechatbot.devin.ChatBot.from_model(\n",
    "    model = openai_model,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools are stored in a `ToolSet` object which is accessed via the `toolset` property. That object stores a dictionary of all tools and their parameters so they can be referenced when a tool call is requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['copy_file',\n",
       " 'file_delete',\n",
       " 'file_search',\n",
       " 'move_file',\n",
       " 'read_file',\n",
       " 'write_file',\n",
       " 'list_directory']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.toolset.names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat History\n",
    "While the LLM itself is just a function, we build conversation-like behavior by storing a chat history. In `simplechatbot`, the history is stored in a `ChatHistory`, which is just a list subtype where list elements contain langchain `BaseMessage` subtypes. You can access it through the `history` property, and work with it just as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the conversation history that is sent to the LLM, you can use the `get_buffer_string` method. This uses the same underlying functions as langchain so you can use this for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(chatbot.history.get_buffer_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Methods\n",
    "\n",
    "The methods `chat` and `chat_stream` both send the provided message with history to the LLM for prediction.\n",
    "\n",
    "+ `chat`: returns `ChatResult` instance with the AI message and ability to call tool functions with provided parameters.\n",
    "+ `chat_stream`: returns `ChatStream` instance which allows you to iterate over responses from the LLM and call tools once all of the parameters have been provided.\n",
    "\n",
    "I will now describe them in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = '''\n",
    "Your job is to answer any questions the user has.\n",
    "'''\n",
    "chatbot = simplechatbot.devin.ChatBot.from_model(\n",
    "    model = openai_model,\n",
    "    system_prompt=system_prompt,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "reply = chatbot.chat('What is the capital of France?')\n",
    "reply.message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ChatResult` Objects\n",
    "\n",
    "These objects are returned from calls to `chat`. Most importantly, access the `message` property to see the response directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 328, 'output_tokens': 9, 'total_tokens': 337}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply.message.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 9,\n",
       "  'prompt_tokens': 328,\n",
       "  'total_tokens': 337,\n",
       "  'completion_tokens_details': {'reasoning_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'accepted_prediction_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}},\n",
       " 'model_name': 'gpt-4o-mini-2024-07-18',\n",
       " 'system_fingerprint': 'fp_0aa8d3e20b',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply.message.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See available tool calls using `tool_calls`. This is a thin wrapper over the `AIMessage.tool_calls` property.\n",
    "\n",
    "See how the question about the capital of France does not require a tool call but the request to save a note does require a tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.chat('What is the capital of France?').tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ToolCallInfo(id='call_ahyOW5qQnm41VFMsCy0rlNdj', name='write_file', type='tool_call', args={'file_path': 'note.txt', 'text': 'Hello, world!'}, tool=WriteFileTool(root_dir='/var/folders/cx/g8dnkjjn3f3dy8r8_8kr12kc0000gn/T/tmp0jm9c49m'))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply = chatbot.chat('Can you save a note to the file \"note.txt\" that says \"Hello, world!\"?')\n",
    "reply.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a call to `call_tools()` to actually execute all of the tools. The results are returned as dicts with tool names as keys pointing to `ToolCallResult` instances, which contain the tool call id, the tool call arguments, a reference to the tool object, and the value returned from the tool, among other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write_file(file_path=note.txt, text=Hello, world!) -> File written successfully to note.txt.\n",
      "ToolCallResult(info=ToolCallInfo(id='call_ahyOW5qQnm41VFMsCy0rlNdj', name='write_file', type='tool_call', args={'file_path': 'note.txt', 'text': 'Hello, world!'}, tool=WriteFileTool(root_dir='/var/folders/cx/g8dnkjjn3f3dy8r8_8kr12kc0000gn/T/tmp0jm9c49m')), return_value='File written successfully to note.txt.')\n"
     ]
    }
   ],
   "source": [
    "tool_call_results = reply.execute_tools()\n",
    "for tool_name, result in tool_call_results.items():\n",
    "    print(result.info.tool_info_str(), '->', result.return_value)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can verify that the LLM actually wrote the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['note.txt']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(working_directory.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ChatStream` Objects\n",
    "\n",
    "Calls to `chat_stream` are very similar but instead return `ChatStream` objects. These objects are iterators and must be iterated over before you can call any tools. If you are not expecting any tool calls, you can call `chat_stream` directly in the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "for r in chatbot.chat_stream(f'What is the capital of France?'):\n",
    "    print(r.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to be able to call tools while streaming, iterate through the reply chunks to completion before executing the tool calls using `call_tools`. In the code below we receive the stream object with the original call to `chat_stream`, iterate through the reply content, and then, when the iterable has been exhausted, call `execute_tools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ToolCallInfo(id='call_HIaCbdz8f4WllYagaRIZuWqA', name='write_file', type='tool_call', args={'file_path': 'hello.txt', 'text': 'You put me here as a test!'}, tool=WriteFileTool(root_dir='/var/folders/cx/g8dnkjjn3f3dy8r8_8kr12kc0000gn/T/tmp0jm9c49m'))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply_stream = chatbot.chat_stream(f'Can you save a note to the file \"hello.txt\" that says \"You put me here as a test!\"?')\n",
    "for r in reply_stream:\n",
    "    print(r.content, end='', flush=True)\n",
    "\n",
    "reply_stream.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'write_file': ToolCallResult(info=ToolCallInfo(id='call_HIaCbdz8f4WllYagaRIZuWqA', name='write_file', type='tool_call', args={'file_path': 'hello.txt', 'text': 'You put me here as a test!'}, tool=WriteFileTool(root_dir='/var/folders/cx/g8dnkjjn3f3dy8r8_8kr12kc0000gn/T/tmp0jm9c49m')), return_value='File written successfully to hello.txt.')}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply_stream.execute_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: do not call `call_tools` before completing the iterable because the chatbot may not have streamed all of the tool call information back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat User Interface\n",
    "Of course, what is a chatbot if you can't actually use it? To run an interactive command-line chat, use `.ui.start_interactive`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to start interactive chat\n",
    "#chatbot.ui.start_interactive(stream=True, show_intro=True, show_tools=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
